{"version":"1","records":[{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy"},"type":"lvl1","url":"/mooreslaw-tutorial","position":0},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy"},"content":"\n\nThe number of transistors reported per a given chip plotted on a log scale in the y axis with the date of introduction on the linear scale x-axis. The blue data points are from a \n\ntransistor count table. The red line is an ordinary least squares prediction and the orange line is Moore’s law.","type":"content","url":"/mooreslaw-tutorial","position":1},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"What you’ll do"},"type":"lvl2","url":"/mooreslaw-tutorial#what-youll-do","position":2},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"What you’ll do"},"content":"In 1965, engineer Gordon Moore\n\n\npredicted that\ntransistors on a chip would double every two years in the coming decade\n[\n\n1].\nYou’ll compare Moore’s prediction against actual transistor counts in\nthe 53 years following his prediction. You will determine the best-fit constants to describe the exponential growth of transistors on semiconductors compared to Moore’s Law.","type":"content","url":"/mooreslaw-tutorial#what-youll-do","position":3},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Skills you’ll learn"},"type":"lvl2","url":"/mooreslaw-tutorial#skills-youll-learn","position":4},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Skills you’ll learn"},"content":"Load data from a \n\n*.csv file\n\nPerform linear regression and predict exponential growth using ordinary least squares\n\nYou’ll compare exponential growth constants between models\n\nShare your analysis in a file:\n\nas NumPy zipped files *.npz\n\nas a *.csv file\n\nAssess the amazing progress semiconductor manufacturers have made in the last five decades","type":"content","url":"/mooreslaw-tutorial#skills-youll-learn","position":5},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"What you’ll need"},"type":"lvl2","url":"/mooreslaw-tutorial#what-youll-need","position":6},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"What you’ll need"},"content":"1. These packages:\n\nNumPy\n\nMatplotlib\n\nimported with the following commands\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n2. Since this is an exponential growth law you need a little background in doing math with \n\nnatural logs and \n\nexponentials.\n\nYou’ll use these NumPy and Matplotlib functions:\n\nnp.loadtxt: this function loads text into a NumPy array\n\nnp.log: this function takes the natural log of all elements in a NumPy array\n\nnp.exp: this function takes the exponential of all elements in a NumPy array\n\nlambda: this is a minimal function definition for creating a function model\n\nplt.semilogy: this function will plot x-y data onto a figure with a linear x-axis and \\log_{10} y-axis\n\n\nplt.plot: this function will plot x-y data on linear axes\n\nslicing arrays: view parts of the data loaded into the workspace, slice the arrays e.g. x[:10] for the first 10 values in the array, x\n\nboolean array indexing: to view parts of the data that match a given condition use boolean operations to index an array\n\nnp.block: to combine arrays into 2D arrays\n\nnp.newaxis: to change a 1D vector to a row or column vector\n\nnp.savez and \n\nnp.savetxt: these two functions will save your arrays in zipped array format and text, respectively\n\n","type":"content","url":"/mooreslaw-tutorial#what-youll-need","position":7},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Building Moore’s law as an exponential function"},"type":"lvl2","url":"/mooreslaw-tutorial#building-moores-law-as-an-exponential-function","position":8},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Building Moore’s law as an exponential function"},"content":"Your empirical model assumes that the number of transistors per\nsemiconductor follows an exponential growth,\n\n\\log(\\text{transistor_count})= f(\\text{year}) = A\\cdot \\text{year}+B,\n\nwhere A and B are fitting constants. You use semiconductor\nmanufacturers’ data to find the fitting constants.\n\nYou determine these constants for Moore’s law by specifying the\nrate for added transistors, 2, and giving an initial number of transistors for a given year.\n\nYou state Moore’s law in an exponential form as follows,\n\n\\text{transistor_count}= e^{A_M\\cdot \\text{year} +B_M}.\n\nWhere A_M and B_M are constants that double the number of transistors every two years and start at 2250 transistors in 1971,\n\n\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = 2 = \\dfrac{e^{B_M}e^{A_M \\text{year} + 2A_M}}{e^{B_M}e^{A_M \\text{year}}} = e^{2A_M} \\rightarrow A_M = \\frac{\\log(2)}{2}\n\n\\log(2250) = \\frac{\\log(2)}{2}\\cdot 1971 + B_M \\rightarrow B_M = \\log(2250)-\\frac{\\log(2)}{2}\\cdot 1971\n\nso Moore’s law stated as an exponential function is\n\n\\log(\\text{transistor_count})= A_M\\cdot \\text{year}+B_M,\n\nwhere\n\nA_M=0.3466\n\nB_M=-675.4\n\nSince the function represents Moore’s law, define it as a Python\nfunction using\n\n\nlambda:\n\nA_M = np.log(2) / 2\nB_M = np.log(2250) - A_M * 1971\nMoores_law = lambda year: np.exp(B_M) * np.exp(A_M * year)\n\nIn 1971, there were 2250 transistors on the Intel 4004 chip. Use\nMoores_law to check the number of semiconductors Gordon Moore would expect\nin 1973.\n\nML_1971 = Moores_law(1971)\nML_1973 = Moores_law(1973)\nprint(\"In 1973, G. Moore expects {:.0f} transistors on Intels chips\".format(ML_1973))\nprint(\"This is x{:.2f} more transistors than 1971\".format(ML_1973 / ML_1971))\n\n","type":"content","url":"/mooreslaw-tutorial#building-moores-law-as-an-exponential-function","position":9},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Loading historical manufacturing data to your workspace"},"type":"lvl2","url":"/mooreslaw-tutorial#loading-historical-manufacturing-data-to-your-workspace","position":10},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Loading historical manufacturing data to your workspace"},"content":"Now, make a prediction based upon the historical data for\nsemiconductors per chip. The \n\nTransistor Count\n[3]\neach year is in the transistor_data.csv file. Before loading a *.csv\nfile into a NumPy array, its a good idea to inspect the structure of the\nfile first. Then, locate the columns of interest and save them to a\nvariable. Save two columns of the file to the array, data.\n\nHere, print out the first 10 rows of transistor_data.csv. The columns are\n\nProcessor\n\nMOS transistor count\n\nDate of Introduction\n\nDesigner\n\nMOSprocess\n\nArea\n\nIntel 4004 (4-bit  16-pin)\n\n2250\n\n1971\n\nIntel\n\n“10,000 nm”\n\n12 mm²\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n! head transistor_data.csv\n\nYou don’t need the columns that specify Processor, Designer,\nMOSprocess, or Area. That leaves the second and third columns,\nMOS transistor count and Date of Introduction, respectively.\n\nNext, you load these two columns into a NumPy array using\n\n\nnp.loadtxt.\nThe extra options below will put the data in the desired format:\n\ndelimiter = ',': specify delimeter as a comma ‘,’ (this is the default behavior)\n\nusecols = [1,2]: import the second and third columns from the csv\n\nskiprows = 1: do not use the first row, because it’s a header row\n\ndata = np.loadtxt(\"transistor_data.csv\", delimiter=\",\", usecols=[1, 2], skiprows=1)\n\nYou loaded the entire history of semiconducting into a NumPy array named\ndata. The first column is the MOS transistor count and the second\ncolumn is the Date of Introduction in a four-digit year.\n\nNext, make the data easier to read and manage by assigning the two\ncolumns to variables, year and transistor_count. Print out the first\n10 values by slicing the year and transistor_count arrays with\n[:10]. Print these values out to check that you have the saved the\ndata to the correct variables.\n\nyear = data[:, 1]  # grab the second column and assign\ntransistor_count = data[:, 0]  # grab the first column and assign\n\nprint(\"year:\\t\\t\", year[:10])\nprint(\"trans. cnt:\\t\", transistor_count[:10])\n\nYou are creating a function that predicts the transistor count given a\nyear. You have an independent variable, year, and a dependent\nvariable, transistor_count. Transform the dependent variable to\nlog-scale,\n\ny_i = \\log( transistor_count[i] ),\n\nresulting in a linear equation,\n\ny_i = A\\cdot \\text{year} +B.\n\nyi = np.log(transistor_count)\n\n","type":"content","url":"/mooreslaw-tutorial#loading-historical-manufacturing-data-to-your-workspace","position":11},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Calculating the historical growth curve for transistors"},"type":"lvl2","url":"/mooreslaw-tutorial#calculating-the-historical-growth-curve-for-transistors","position":12},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Calculating the historical growth curve for transistors"},"content":"Your model assume that yi is a function of year. Now, find the best-fit model that minimizes the difference between y_i and A\\cdot \\text{year} +B,  as such\n\n\\min \\sum|y_i - (A\\cdot \\text{year}_i + B)|^2.\n\nThis \n\nsum of squares\nerror can be\nsuccinctly represented as arrays as such\n\n\\sum|\\mathbf{y}-\\mathbf{Z} [A,~B]^T|^2,\n\nwhere \\mathbf{y} are the observations of the log of the number of\ntransistors in a 1D array and \\mathbf{Z}=[\\text{year}_i^1,~\\text{year}_i^0] are the\npolynomial terms for \\text{year}_i in the first and second columns. By\ncreating this set of regressors in the \\mathbf{Z}-matrix you set\nup an ordinary least squares statistical model.\n\nZ is a linear model with two parameters, i.e. a polynomial with degree 1.\nTherefore we can represent the model with numpy.polynomial.Polynomial and\nuse the fitting functionality to determine the model parameters:\n\nmodel = np.polynomial.Polynomial.fit(year, yi, deg=1)\n\nBy default, Polynomial.fit performs the fit in the domain determined by the\nindependent variable (year in this case).\nThe coefficients for the unscaled and unshifted model can be recovered with the\nconvert method:\n\nmodel = model.convert()\nmodel\n\nThe individual parameters A and B are the coefficients of our linear model:\n\nB, A = model\n\nDid manufacturers double the transistor count every two years? You have\nthe final formula,\n\n\\dfrac{\\text{transistor_count}(\\text{year} +2)}{\\text{transistor_count}(\\text{year})} = xFactor =\n\\dfrac{e^{B}e^{A( \\text{year} + 2)}}{e^{B}e^{A \\text{year}}} = e^{2A}\n\nwhere increase in number of transistors is xFactor, number of years is\n2, and A is the best fit slope on the semilog function.\n\nprint(f\"Rate of semiconductors added on a chip every 2 years: {np.exp(2 * A):.2f}\")\n\nBased upon your least-squares regression model, the number of\nsemiconductors per chip increased by a factor of 1.98 every two\nyears. You have a model that predicts the number of semiconductors each\nyear. Now compare your model to the actual manufacturing reports.  Plot\nthe linear regression results and all of the transistor counts.\n\nHere, use\n\n\nplt.semilogy\nto plot the number of transistors on a log-scale and the year on a\nlinear scale. You have defined a three arrays to get to a final model\n\ny_i = \\log(\\text{transistor_count}),\n\ny_i = A \\cdot \\text{year} + B,\n\nand\n\n\\log(\\text{transistor_count}) = A\\cdot \\text{year} + B,\n\nyour variables, transistor_count, year, and yi all have the same\ndimensions, (179,). NumPy arrays need the same dimensions to make a\nplot. The predicted number of transistors is now\n\n\\text{transistor_count}_{\\text{predicted}} = e^Be^{A\\cdot \\text{year}}.\n\nIn the next plot, use the\n\n\nfivethirtyeight\nstyle sheet.\nThe style sheet replicates\n\n\nhttps://​fivethirtyeight​.com elements. Change the matplotlib style with\n\n\nplt.style.use.\n\ntransistor_count_predicted = np.exp(B) * np.exp(A * year)\ntransistor_Moores_law = Moores_law(year)\nplt.style.use(\"fivethirtyeight\")\nplt.semilogy(year, transistor_count, \"s\", label=\"MOS transistor count\")\nplt.semilogy(year, transistor_count_predicted, label=\"linear regression\")\n\n\nplt.plot(year, transistor_Moores_law, label=\"Moore's Law\")\nplt.title(\n    \"MOS transistor count per microprocessor\\n\"\n    + \"every two years \\n\"\n    + \"Transistor count was x{:.2f} higher\".format(np.exp(A * 2))\n)\nplt.xlabel(\"year introduced\")\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"# of transistors\\nper microprocessor\")\n\nA scatter plot of MOS transistor count per microprocessor every two years with a red line for the ordinary least squares prediction and an orange line for Moore’s law.\n\nThe linear regression captures the increase in the number of transistors\nper semiconductors each year.  In 2015, semiconductor manufacturers\nclaimed they could not keep up with Moore’s law anymore. Your analysis\nshows that since 1971, the average increase in transistor count was\nx1.98 every 2 years, but Gordon Moore predicted it would be x2\nevery 2 years. That is an amazing prediction.\n\nConsider the year 2017. Compare the data to your linear regression\nmodel and Gordon Moore’s prediction. First, get the\ntransistor counts from the year 2017. You can do this with a Boolean\ncomparator,\n\nyear == 2017.\n\nThen, make a prediction for 2017 with Moores_law defined above\nand plugging in your best fit constants into your function\n\n\\text{transistor_count} = e^{B}e^{A\\cdot \\text{year}}.\n\nA great way to compare these measurements is to compare your prediction\nand Moore’s prediction to the average transistor count and look at the\nrange of reported values for that year. Use the\n\n\nplt.plot\noption,\n\n\nalpha=0.2,\nto increase the transparency of the data. The more opaque the points\nappear, the more reported values lie on that measurement. The green +\nis the average reported transistor count for 2017. Plot your predictions\nfor \\pm\\frac{1}{2} years.\n\ntransistor_count2017 = transistor_count[year == 2017]\nprint(\n    transistor_count2017.max(), transistor_count2017.min(), transistor_count2017.mean()\n)\ny = np.linspace(2016.5, 2017.5)\nyour_model2017 = np.exp(B) * np.exp(A * y)\nMoore_Model2017 = Moores_law(y)\n\nplt.plot(\n    2017 * np.ones(np.sum(year == 2017)),\n    transistor_count2017,\n    \"ro\",\n    label=\"2017\",\n    alpha=0.2,\n)\nplt.plot(2017, transistor_count2017.mean(), \"g+\", markersize=20, mew=6)\n\nplt.plot(y, your_model2017, label=\"Your prediction\")\nplt.plot(y, Moore_Model2017, label=\"Moores law\")\nplt.ylabel(\"# of transistors\\nper microprocessor\")\nplt.legend()\n\nThe result is that your model is close to the mean, but Gordon\nMoore’s prediction is closer to the maximum number of transistors per\nmicroprocessor produced in 2017. Even though semiconductor manufacturers\nthought that the growth would slow, once in 1975 and now again\napproaching 2025, manufacturers are still producing semiconductors every 2 years that\nnearly double the number of transistors.\n\nThe linear regression model is much better at predicting the\naverage than extreme values because it satisfies the condition to\nminimize \\sum |y_i - A\\cdot \\text{year}[i]+B|^2.\n\n","type":"content","url":"/mooreslaw-tutorial#calculating-the-historical-growth-curve-for-transistors","position":13},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Sharing your results as zipped arrays and a csv"},"type":"lvl2","url":"/mooreslaw-tutorial#sharing-your-results-as-zipped-arrays-and-a-csv","position":14},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Sharing your results as zipped arrays and a csv"},"content":"The last step, is to share your findings. You created\nnew arrays that represent a linear regression model and Gordon Moore’s\nprediction. You started this process by importing a csv file into a NumPy\narray using np.loadtxt, to save your model use two approaches\n\nnp.savez: save NumPy arrays for other Python sessions\n\nnp.savetxt: save a csv file with the original data and your predicted data","type":"content","url":"/mooreslaw-tutorial#sharing-your-results-as-zipped-arrays-and-a-csv","position":15},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl3":"Zipping the arrays into a file","lvl2":"Sharing your results as zipped arrays and a csv"},"type":"lvl3","url":"/mooreslaw-tutorial#zipping-the-arrays-into-a-file","position":16},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl3":"Zipping the arrays into a file","lvl2":"Sharing your results as zipped arrays and a csv"},"content":"Using np.savez, you can save thousands of arrays and give them names. The\nfunction np.load will load the arrays back into the workspace as a\ndictionary. You’ll save five arrays so the next user will have the year,\ntransistor count, predicted transistor count,  Gordon Moore’s\npredicted count, and fitting constants. Add one more variable that other users can use to\nunderstand the model, notes.\n\nnotes = \"the arrays in this file are the result of a linear regression model\\n\"\nnotes += \"the arrays include\\nyear: year of manufacture\\n\"\nnotes += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nnotes += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nnotes += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nnotes += \"regression_csts: linear regression constants A and B for log(transistor_count)=A*year+B\"\nprint(notes)\n\n\n\nnp.savez(\n    \"mooreslaw_regression.npz\",\n    notes=notes,\n    year=year,\n    transistor_count=transistor_count,\n    transistor_count_predicted=transistor_count_predicted,\n    transistor_Moores_law=transistor_Moores_law,\n    regression_csts=(A, B),\n)\n\n\n\nresults = np.load(\"mooreslaw_regression.npz\")\n\n\n\nprint(results[\"regression_csts\"][1])\n\n\n\n! ls\n\nThe benefit of np.savez is you can save hundreds of arrays with\ndifferent shapes and types. Here, you saved 4 arrays that are double\nprecision floating point numbers shape = (179,), one array that was\ntext, and one array of double precision floating point numbers shape =\n(2,). This is the preferred method for saving NumPy arrays for use in\nanother analysis.","type":"content","url":"/mooreslaw-tutorial#zipping-the-arrays-into-a-file","position":17},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl3":"Creating your own comma separated value file","lvl2":"Sharing your results as zipped arrays and a csv"},"type":"lvl3","url":"/mooreslaw-tutorial#creating-your-own-comma-separated-value-file","position":18},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl3":"Creating your own comma separated value file","lvl2":"Sharing your results as zipped arrays and a csv"},"content":"If you want to share data and view the results in a table, then you have to\ncreate a text file. Save the data using np.savetxt. This\nfunction is more limited than np.savez. Delimited files, like csv’s,\nneed 2D arrays.\n\nPrepare the data for export by creating a new 2D array whose columns\ncontain the data of interest.\n\nUse the header option to describe the data and the columns of\nthe file. Define another variable that contains file\ninformation as head.\n\nhead = \"the columns in this file are the result of a linear regression model\\n\"\nhead += \"the columns include\\nyear: year of manufacture\\n\"\nhead += \"transistor_count: number of transistors reported by manufacturers in a given year\\n\"\nhead += \"transistor_count_predicted: linear regression model = exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B, A\n)\nhead += \"transistor_Moores_law: Moores law =exp({:.2f})*exp({:.2f}*year)\\n\".format(\n    B_M, A_M\n)\nhead += \"year:, transistor_count:, transistor_count_predicted:, transistor_Moores_law:\"\nprint(head)\n\nBuild a single 2D array to export to csv. Tabular data is inherently two\ndimensional. You need to organize your data to fit this 2D structure.\nUse year, transistor_count, transistor_count_predicted, and\ntransistor_Moores_law as the first through fourth columns,\nrespectively. Put the calculated constants in the header since they do\nnot fit the (179,) shape. The\n\n\nnp.block\nfunction appends arrays together to create a new, larger array. Arrange\nthe 1D vectors as columns using\n\n\nnp.newaxis\ne.g.>>> year.shape\n(179,)\n>>> year[:,np.newaxis].shape\n(179,1)\n\noutput = np.block(\n    [\n        year[:, np.newaxis],\n        transistor_count[:, np.newaxis],\n        transistor_count_predicted[:, np.newaxis],\n        transistor_Moores_law[:, np.newaxis],\n    ]\n)\n\nCreating the mooreslaw_regression.csv with np.savetxt, use three\noptions to create the desired file format:\n\nX = output : use output block to write the data into the file\n\ndelimiter = ',' : use commas to separate columns in the file\n\nheader = head : use the header head defined above\n\nnp.savetxt(\"mooreslaw_regression.csv\", X=output, delimiter=\",\", header=head)\n\n\n\n! head mooreslaw_regression.csv\n\n","type":"content","url":"/mooreslaw-tutorial#creating-your-own-comma-separated-value-file","position":19},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Wrapping up"},"type":"lvl2","url":"/mooreslaw-tutorial#wrapping-up","position":20},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"Wrapping up"},"content":"In conclusion, you have compared historical data for semiconductor\nmanufacturers to Moore’s law and created a linear regression model to\nfind the average number of transistors added to each microprocessor\nevery two years. Gordon Moore predicted the number of transistors would\ndouble every two years from 1965 through 1975, but the average growth\nhas maintained a consistent increase of \\times 1.98 \\pm 0.01 every two\nyears from 1971 through 2019.  In 2015, Moore revised his prediction to\nsay Moore’s law should hold until 2025.\n[\n\n2].\nYou can share these results as a zipped NumPy array file,\nmooreslaw_regression.npz, or as another csv,\nmooreslaw_regression.csv.  The amazing progress in semiconductor\nmanufacturing has enabled new industries and computational power. This\nanalysis should give you a small insight into how incredible this growth\nhas been over the last half-century.\n\n","type":"content","url":"/mooreslaw-tutorial#wrapping-up","position":21},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"References"},"type":"lvl2","url":"/mooreslaw-tutorial#references","position":22},{"hierarchy":{"lvl1":"Determining Moore’s Law with real data in NumPy","lvl2":"References"},"content":"“Moore’s Law.” Wikipedia article. Accessed Oct. 1, 2020.\n\nCourtland, Rachel. “Gordon Moore: The Man Whose Name Means Progress.” IEEE Spectrum. 30 Mar. 2015..\n\n“Transistor Count.” Wikipedia article. Accessed Oct. 1, 2020.","type":"content","url":"/mooreslaw-tutorial#references","position":23},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays"},"type":"lvl1","url":"/save-load-arrays","position":0},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays"},"content":"","type":"content","url":"/save-load-arrays","position":1},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"What you’ll learn"},"type":"lvl2","url":"/save-load-arrays#what-youll-learn","position":2},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"What you’ll learn"},"content":"You’ll save your NumPy arrays as zipped files and human-readable\ncomma-delimited files i.e. *.csv. You will also learn to load both of these\nfile types back into NumPy workspaces.","type":"content","url":"/save-load-arrays#what-youll-learn","position":3},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"What you’ll do"},"type":"lvl2","url":"/save-load-arrays#what-youll-do","position":4},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"What you’ll do"},"content":"You’ll learn two ways of saving and reading files--as compressed and as\ntext files--that will serve most of your storage needs in NumPy.\n\nYou’ll create two 1D arrays and one 2D array\n\nYou’ll save these arrays to files\n\nYou’ll remove variables from your workspace\n\nYou’ll load the variables from your saved file\n\nYou’ll compare zipped binary files to human-readable delimited files\n\nYou’ll finish with the skills of saving, loading, and sharing NumPy arrays","type":"content","url":"/save-load-arrays#what-youll-do","position":5},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"What you’ll need"},"type":"lvl2","url":"/save-load-arrays#what-youll-need","position":6},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"What you’ll need"},"content":"NumPy\n\nread-write access to your working directory\n\nLoad the necessary functions using the following command.\n\nimport numpy as np\n\nIn this tutorial, you will use the following Python, IPython magic, and NumPy functions:\n\nnp.arange\n\nnp.savez\n\ndel\n\nwhos\n\nnp.load\n\nnp.block\n\nnp.newaxis\n\nnp.savetxt\n\nnp.loadtxt\n\n","type":"content","url":"/save-load-arrays#what-youll-need","position":7},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Create your arrays"},"type":"lvl2","url":"/save-load-arrays#create-your-arrays","position":8},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Create your arrays"},"content":"Now that you have imported the NumPy library, you can make a couple of\narrays; let’s start with two 1D arrays, x and y, where y = x**2.You\nwill assign x to the integers from 0 to 9 using\n\n\nnp.arange.\n\nx = np.arange(10)\ny = x ** 2\nprint(x)\nprint(y)\n\n","type":"content","url":"/save-load-arrays#create-your-arrays","position":9},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Save your arrays with NumPy’s savez"},"type":"lvl2","url":"/save-load-arrays#save-your-arrays-with-numpys-savez","position":10},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Save your arrays with NumPy’s savez"},"content":"Now you have two arrays in your workspace,\n\nx: [0 1 2 3 4 5 6 7 8 9]\n\ny: [ 0  1  4  9 16 25 36 49 64 81]\n\nThe first thing you will do is save them to a file as zipped arrays\nusing\n\n\nsavez.\nYou will use two options to label the arrays in the file,\n\nx_axis = x: this option is assigning the name x_axis to the variable x\n\ny_axis = y: this option is assigning the name y_axis to the variable y\n\nnp.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)\n\n","type":"content","url":"/save-load-arrays#save-your-arrays-with-numpys-savez","position":11},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Remove the saved arrays and load them back with NumPy’s load"},"type":"lvl2","url":"/save-load-arrays#remove-the-saved-arrays-and-load-them-back-with-numpys-load","position":12},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Remove the saved arrays and load them back with NumPy’s load"},"content":"In your current working directory, you should have a new file with the\nname x_y-squared.npz. This file is a zipped binary of the two arrays,\nx and y. Let’s clear the workspace and load the values back in. This\nx_y-squared.npz file contains two \n\nNPY\nformat\nfiles. The NPY format is a \n\nnative binary\nformat. You cannot read\nthe numbers in a standard text editor or spreadsheet.\n\nremove x and y from the workspaec with \n\ndel\n\nload the arrays into the workspace in a dictionary with \n\nnp.load\n\nTo see what variables are in the workspace, use the Jupyter/IPython\n“magic” command\n\n\nwhos.\n\ndel x, y\n\n\n\n%whos\n\n\n\nload_xy = np.load(\"x_y-squared.npz\")\n\nprint(load_xy.files)\n\n\n\n%whos\n\n","type":"content","url":"/save-load-arrays#remove-the-saved-arrays-and-load-them-back-with-numpys-load","position":13},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Reassign the NpzFile arrays to x and y"},"type":"lvl2","url":"/save-load-arrays#reassign-the-npzfile-arrays-to-x-and-y","position":14},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Reassign the NpzFile arrays to x and y"},"content":"You’ve now created the dictionary with an NpzFile-type. The\nincluded files are x_axis and y_axis that you defined in your\nsavez command. You can reassign x and y to the load_xy files.\n\nx = load_xy[\"x_axis\"]\ny = load_xy[\"y_axis\"]\nprint(x)\nprint(y)\n\n","type":"content","url":"/save-load-arrays#reassign-the-npzfile-arrays-to-x-and-y","position":15},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Success"},"type":"lvl2","url":"/save-load-arrays#success","position":16},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Success"},"content":"You have created, saved, deleted, and loaded the variables x and y using savez and load. Nice work.","type":"content","url":"/save-load-arrays#success","position":17},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Another option: saving to human-readable csv"},"type":"lvl2","url":"/save-load-arrays#another-option-saving-to-human-readable-csv","position":18},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Another option: saving to human-readable csv"},"content":"Let’s consider another scenario, you want to share x and y with\nother people or other programs. You may need human-readable text file\nthat is easier to share. Next, you use the\n\n\nsavetxt\nto save x and y in a comma separated value file, x_y-squared.csv.\nThe resulting csv is composed of ASCII characters. You can load the file\nback into NumPy or read it with other programs.","type":"content","url":"/save-load-arrays#another-option-saving-to-human-readable-csv","position":19},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Rearrange the data into a single 2D array"},"type":"lvl2","url":"/save-load-arrays#rearrange-the-data-into-a-single-2d-array","position":20},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Rearrange the data into a single 2D array"},"content":"First, you have to create a single 2D array from your two 1D arrays. The\ncsv-filetype is a spreadsheet-style dataset. The csv arranges numbers in\nrows--separated by new lines--and columns--separated by commas. If the\ndata is more complex e.g. multiple 2D arrays or higher dimensional\narrays, it is better to use savez. Here, you use\ntwo NumPy functions to format the data:\n\nnp.block: this function appends arrays together into a 2D array\n\nnp.newaxis: this function forces the 1D array into a 2D column vector with 10 rows and 1 column.\n\narray_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)\n\n","type":"content","url":"/save-load-arrays#rearrange-the-data-into-a-single-2d-array","position":21},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Save the data to csv file using savetxt"},"type":"lvl2","url":"/save-load-arrays#save-the-data-to-csv-file-using-savetxt","position":22},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Save the data to csv file using savetxt"},"content":"You use savetxt with a three options to make your file easier to read:\n\nX = array_out: this option tells savetxt to save your 2D array, array_out, to the file x_y-squared.csv\n\nheader = 'x, y': this option writes a header before any data that labels the columns of the csv\n\ndelimiter = ',': this option tells savetxt to place a comma between each column in the file\n\nnp.savetxt(\"x_y-squared.csv\", X=array_out, header=\"x, y\", delimiter=\",\")\n\nOpen the file, x_y-squared.csv, and you’ll see the following:\n\n!head x_y-squared.csv\n\n","type":"content","url":"/save-load-arrays#save-the-data-to-csv-file-using-savetxt","position":23},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Our arrays as a csv file"},"type":"lvl2","url":"/save-load-arrays#our-arrays-as-a-csv-file","position":24},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Our arrays as a csv file"},"content":"There are two features that you shoud notice here:\n\nNumPy uses # to ignore headings when using loadtxt. If you’re using\n\n\nloadtxt\nwith other csv files, you can skip header rows with skiprows = <number_of_header_lines>.\n\nThe integers were written in scientific notation. You can specify\nthe format of the text using the savetxt option, \n\nfmt = , but it\nwill still be written with ASCII characters. In general, you cannot\npreserve the type of ASCII numbers as float or int.\n\nNow, delete x and y again and assign them to your columns in x-y_squared.csv.\n\ndel x, y\n\n\n\nload_xy = np.loadtxt(\"x_y-squared.csv\", delimiter=\",\")\n\n\n\nload_xy.shape\n\n\n\nx = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)\n\n","type":"content","url":"/save-load-arrays#our-arrays-as-a-csv-file","position":25},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Success, but remember your types"},"type":"lvl2","url":"/save-load-arrays#success-but-remember-your-types","position":26},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Success, but remember your types"},"content":"When you saved the arrays to the csv file, you did not preserve the\nint type. When loading the arrays back into your workspace the default process will be to load the csv file as a 2D floating point array e.g. load_xy.dtype == 'float64' and load_xy.shape == (10, 2).\n\n","type":"content","url":"/save-load-arrays#success-but-remember-your-types","position":27},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Wrapping up"},"type":"lvl2","url":"/save-load-arrays#wrapping-up","position":28},{"hierarchy":{"lvl1":"Saving and sharing your NumPy arrays","lvl2":"Wrapping up"},"content":"In conclusion, you can create, save, and load arrays in NumPy. Saving arrays makes sharing your work and collaboration much easier. There are other ways Python can save data to files, such as \n\npickle, but savez and savetxt will serve most of your storage needs for future NumPy work and sharing with other people, respectively.\n\nNext steps: you can import data with missing values from \n\nImporting with genfromtext or learn more about general NumPy IO with \n\nReading and Writing Files.","type":"content","url":"/save-load-arrays#wrapping-up","position":29},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India"},"type":"lvl1","url":"/tutorial-air-quality-analysis","position":0},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India"},"content":"","type":"content","url":"/tutorial-air-quality-analysis","position":1},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What you’ll do"},"type":"lvl2","url":"/tutorial-air-quality-analysis#what-youll-do","position":2},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What you’ll do"},"content":"Calculate Air Quality Indices (AQI) and perform paired Student’s t-test on them.","type":"content","url":"/tutorial-air-quality-analysis#what-youll-do","position":3},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What you’ll learn"},"type":"lvl2","url":"/tutorial-air-quality-analysis#what-youll-learn","position":4},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What you’ll learn"},"content":"You’ll learn the concept of moving averages\n\nYou’ll learn how to calculate Air Quality Index (AQI)\n\nYou’ll learn how to perform a paired Student’s t-test and find the t and p values\n\nYou’ll learn how to interpret these values","type":"content","url":"/tutorial-air-quality-analysis#what-youll-learn","position":5},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What you’ll need"},"type":"lvl2","url":"/tutorial-air-quality-analysis#what-youll-need","position":6},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What you’ll need"},"content":"SciPy installed in your environment\n\nBasic understanding of statistical terms like population, sample, mean, standard deviation etc.\n\n","type":"content","url":"/tutorial-air-quality-analysis#what-youll-need","position":7},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"The problem of air pollution"},"type":"lvl2","url":"/tutorial-air-quality-analysis#the-problem-of-air-pollution","position":8},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"The problem of air pollution"},"content":"Air pollution is one of the most prominent types of pollution we face that has an immediate effect on our daily lives. The\nCOVID-19 pandemic resulted in lockdowns in different parts of the world; offering a rare opportunity to study the effect of\nhuman activity (or lack thereof) on air pollution. In this tutorial, we will study the air quality in Delhi, one of the\nworst affected cities by air pollution, before and during the lockdown from March to June 2020. For this, we will first compute\nthe Air Quality Index for each hour from the collected pollutant measurements. Next, we will sample these indices and perform\na \n\npaired Student’s t-test on them. It will statistically show us that the air quality improved due to the lockdown, supporting our intuition.\n\nLet’s start by importing the necessary libraries into our environment.\n\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy import stats\n\n","type":"content","url":"/tutorial-air-quality-analysis#the-problem-of-air-pollution","position":9},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Building the dataset"},"type":"lvl2","url":"/tutorial-air-quality-analysis#building-the-dataset","position":10},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Building the dataset"},"content":"We will use a condensed version of the \n\nAir Quality Data in India dataset. This dataset contains air quality data and AQI (Air Quality Index) at hourly and daily level of various stations across multiple cities in India. The condensed version available with this tutorial contains hourly pollutant measurements for Delhi\nfrom May 31, 2019 to June 30, 2020. It has measurements of the standard pollutants that are required for Air Quality Index calculation and a few other important ones:\nParticulate Matter (PM 2.5 and PM 10), nitrogen dioxide (NO2), ammonia (NH3), sulfur dioxide (SO2), carbon monoxide (CO), ozone (O3), oxides of nitrogen (NOx), nitric oxide (NO), benzene, toluene, and xylene.\n\nLet’s print out the first few rows to have a glimpse of our dataset.\n\n! head air-quality-data.csv\n\nFor the purpose of this tutorial, we are only concerned with standard pollutants required for calculating the AQI, viz., PM 2.5, PM 10, NO2, NH3, SO2, CO, and O3. So, we will only import these particular columns with \n\nnp.loadtxt. We’ll then \n\nslice and create two sets: pollutants_A with PM 2.5, PM 10, NO2, NH3, and SO2, and pollutants_B with CO and O3. The\ntwo sets will be processed slightly differently, as we’ll see later on.\n\npollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)\n\nOur dataset might contain missing values, denoted by NaN, so let’s do a quick check with \n\nnp.isfinite.\n\nnp.all(np.isfinite(pollutant_data))\n\nWith this, we have successfully imported the data and checked that it is complete. Let’s move on to the AQI calculations!\n\n","type":"content","url":"/tutorial-air-quality-analysis#building-the-dataset","position":11},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Calculating the Air Quality Index"},"type":"lvl2","url":"/tutorial-air-quality-analysis#calculating-the-air-quality-index","position":12},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Calculating the Air Quality Index"},"content":"We will calculate the AQI using \n\nthe method adopted by the \n\nCentral Pollution Control Board of India.  To summarize the steps:\n\nCollect 24-hourly average concentration values for the standard pollutants; 8-hourly in case of CO and O3.\n\nCalculate the sub-indices for these pollutants with the formula:Ip = \\dfrac{\\text{IHi – ILo}}{\\text{BPHi – BPLo}}\\cdot{\\text{Cp – BPLo}} + \\text{ILo}\n\nWhere,\n\nIp = sub-index of pollutant pCp = averaged concentration of pollutant pBPHi = concentration breakpoint i.e. greater than or equal to CpBPLo = concentration breakpoint i.e. less than or equal to CpIHi = AQI value corresponding to BPHiILo = AQI value corresponding to BPLo\n\nThe maximum sub-index at any given time is the Air Quality Index.\n\nThe Air Quality Index is calculated with the help of breakpoint ranges as shown in the chart below.\n\nLet’s create two arrays to store the AQI ranges and breakpoints so that we can use them later for our calculations.\n\nAQI = np.array([0, 51, 101, 201, 301, 401, 501])\n\nbreakpoints = {\n    'PM2.5': np.array([0, 31, 61, 91, 121, 251]),\n    'PM10': np.array([0, 51, 101, 251, 351, 431]),\n    'NO2': np.array([0, 41, 81, 181, 281, 401]),\n    'NH3': np.array([0, 201, 401, 801, 1201, 1801]),\n    'SO2': np.array([0, 41, 81, 381, 801, 1601]),\n    'CO': np.array([0, 1.1, 2.1, 10.1, 17.1, 35]),\n    'O3': np.array([0, 51, 101, 169, 209, 749])\n}\n\n","type":"content","url":"/tutorial-air-quality-analysis#calculating-the-air-quality-index","position":13},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Moving averages","lvl2":"Calculating the Air Quality Index"},"type":"lvl3","url":"/tutorial-air-quality-analysis#moving-averages","position":14},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Moving averages","lvl2":"Calculating the Air Quality Index"},"content":"For the first step, we have to compute \n\nmoving averages for pollutants_A over a window of 24 hours and pollutants_B over a\nwindow of 8 hours. We will write a simple function moving_mean using \n\nnp.cumsum and \n\nsliced indexing to achieve this.\n\nTo make sure both the sets are of the same length, we will truncate the pollutants_B_8hr_avg according to the length of\npollutants_A_24hr_avg. This will also ensure we have concentrations for all the pollutants over the same period of time.\n\ndef moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]\n\nNow, we can join both sets with \n\nnp.concatenate to form a single data set of all the averaged concentrations. Note that we have to join our arrays column-wise so we pass the\naxis=1 parameter.\n\npollutants = np.concatenate((pollutants_A_24hr_avg, pollutants_B_8hr_avg), axis=1)\n\n","type":"content","url":"/tutorial-air-quality-analysis#moving-averages","position":15},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Sub-indices","lvl2":"Calculating the Air Quality Index"},"type":"lvl3","url":"/tutorial-air-quality-analysis#sub-indices","position":16},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Sub-indices","lvl2":"Calculating the Air Quality Index"},"content":"The subindices for each pollutant are calculated according to the linear relationship between the AQI and standard breakpoint ranges with the formula as above:Ip = \\dfrac{\\text{IHi – ILo}}{\\text{BPHi – BPLo}}\\cdot{\\text{Cp – BPLo}} + \\text{ILo}\n\nThe compute_indices function first fetches the correct upper and lower bounds of AQI categories and breakpoint concentrations for the input concentration and pollutant with the help of arrays AQI and breakpoints we created above. Then, it feeds these values into the formula to calculate the sub-index.\n\ndef compute_indices(pol, con):\n    bp = breakpoints[pol]\n    \n    if pol == 'CO':\n        inc = 0.1\n    else:\n        inc = 1\n    \n    if bp[0] <= con < bp[1]:\n        Bl = bp[0]\n        Bh = bp[1] - inc\n        Ih = AQI[1] - inc\n        Il = AQI[0]\n\n    elif bp[1] <= con < bp[2]:\n        Bl = bp[1]\n        Bh = bp[2] - inc\n        Ih = AQI[2] - inc\n        Il = AQI[1]\n\n    elif bp[2] <= con < bp[3]:\n        Bl = bp[2]\n        Bh = bp[3] - inc\n        Ih = AQI[3] - inc\n        Il = AQI[2]\n\n    elif bp[3] <= con < bp[4]:\n        Bl = bp[3]\n        Bh = bp[4] - inc\n        Ih = AQI[4] - inc\n        Il = AQI[3]\n\n    elif bp[4] <= con < bp[5]:\n        Bl = bp[4]\n        Bh = bp[5] - inc\n        Ih = AQI[5] - inc\n        Il = AQI[4]\n\n    elif bp[5] <= con:\n        Bl = bp[5]\n        Bh = bp[5] + bp[4] - (2 * inc)\n        Ih = AQI[6]\n        Il = AQI[5]\n\n    else:\n        print(\"Concentration out of range!\")\n        \n    return ((Ih - Il) / (Bh - Bl)) * (con - Bl) + Il\n\nWe will use \n\nnp.vectorize to utilize the concept of vectorization. This simply means we don’t have loop over each element of the pollutant array ourselves. \n\nVectorization is one of the key advantages of NumPy.\n\nvcompute_indices = np.vectorize(compute_indices)\n\nBy calling our vectorized function vcompute_indices for each pollutant, we get the sub-indices. To get back an array with the original shape, we use \n\nnp.stack.\n\nsub_indices = np.stack((vcompute_indices('PM2.5', pollutants[..., 0]),\n                        vcompute_indices('PM10', pollutants[..., 1]),\n                        vcompute_indices('NO2', pollutants[..., 2]),\n                        vcompute_indices('NH3', pollutants[..., 3]),\n                        vcompute_indices('SO2', pollutants[..., 4]),\n                        vcompute_indices('CO', pollutants[..., 5]),\n                        vcompute_indices('O3', pollutants[..., 6])), axis=1)\n\n","type":"content","url":"/tutorial-air-quality-analysis#sub-indices","position":17},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Air quality indices","lvl2":"Calculating the Air Quality Index"},"type":"lvl3","url":"/tutorial-air-quality-analysis#air-quality-indices","position":18},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Air quality indices","lvl2":"Calculating the Air Quality Index"},"content":"Using \n\nnp.max, we find out the maximum sub-index for each period, which is our Air Quality Index!\n\naqi_array = np.max(sub_indices, axis=1)\n\nWith this, we have the AQI for every hour from June 1, 2019 to June 30, 2020. Note that even though we started out with\nthe data from 31st May, we truncated that during the moving averages step.\n\n","type":"content","url":"/tutorial-air-quality-analysis#air-quality-indices","position":19},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Paired Student’s t-test on the AQIs"},"type":"lvl2","url":"/tutorial-air-quality-analysis#paired-students-t-test-on-the-aqis","position":20},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Paired Student’s t-test on the AQIs"},"content":"Hypothesis testing is a form of descriptive statistics used to help us make decisions with the data. From the calculated AQI data, we want to find out if there was a statistically significant difference in average AQI before and after the lockdown was imposed. We will use the left-tailed, \n\npaired Student’s t-test to compute two test statistics- the \n\nt statistic and the \n\np value. We will then compare these with the corresponding critical values to make a decision.","type":"content","url":"/tutorial-air-quality-analysis#paired-students-t-test-on-the-aqis","position":21},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Sampling","lvl2":"Paired Student’s t-test on the AQIs"},"type":"lvl3","url":"/tutorial-air-quality-analysis#sampling","position":22},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Sampling","lvl2":"Paired Student’s t-test on the AQIs"},"content":"We will now import the datetime column from our original dataset into a \n\ndatetime64 dtype array. We will use this array to index the AQI array and obtain subsets of the dataset.\n\ndatetime = np.loadtxt(\"air-quality-data.csv\", dtype='M8[h]', delimiter=\",\",\n                         skiprows=1, usecols=(0, ))[-(pollutants_A_24hr_avg.shape[0]):]\n\nSince total lockdown commenced in Delhi from March 24, 2020, the after-lockdown subset is of the period March 24, 2020 to June 30, 2020. The before-lockdown subset is for the same length of time before 24th March.\n\nafter_lock = aqi_array[np.where(datetime >= np.datetime64('2020-03-24T00'))]\n\nbefore_lock = aqi_array[np.where(datetime <= np.datetime64('2020-03-21T00'))][-(after_lock.shape[0]):]\n\nprint(after_lock.shape)\nprint(before_lock.shape)\n\nTo make sure our samples are approximately normally distributed, we take samples of size n = 30. before_sample and after_sample are the set of random observations drawn before and after the total lockdown. We use \n\nrandom​.Generator​.choice to generate the samples.\n\nrng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)\n\n","type":"content","url":"/tutorial-air-quality-analysis#sampling","position":23},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Defining the hypothesis","lvl2":"Paired Student’s t-test on the AQIs"},"type":"lvl3","url":"/tutorial-air-quality-analysis#defining-the-hypothesis","position":24},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Defining the hypothesis","lvl2":"Paired Student’s t-test on the AQIs"},"content":"Let us assume that there is no significant difference between the sample means before and after the lockdown. This will be the null hypothesis. The alternative hypothesis would be that there is a significant difference between the means and the AQI improved. Mathematically,\n\nH_{0}: \\mu_\\text{after-before} = 0 H_{a}: \\mu_\\text{after-before} < 0\n\n","type":"content","url":"/tutorial-air-quality-analysis#defining-the-hypothesis","position":25},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Calculating the test statistics","lvl2":"Paired Student’s t-test on the AQIs"},"type":"lvl3","url":"/tutorial-air-quality-analysis#calculating-the-test-statistics","position":26},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl3":"Calculating the test statistics","lvl2":"Paired Student’s t-test on the AQIs"},"content":"We will use the t statistic to evaluate our hypothesis and even calculate the p value from it. The formula for the t statistic is:t = \\frac{\\mu_\\text{after-before}}{\\sqrt{\\sigma^{2}/n}}\n\nwhere,\n\n\\mu_\\text{after-before} = mean differences of samples \\sigma^{2} = variance of mean differences n = sample size\n\ndef t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)\n\nFor the p value, we will use SciPy’s stats.distributions.t.cdf() function. It takes two arguments- the t statistic and the degrees of freedom (dof). The formula for dof is n - 1.\n\ndof = len(before_sample) - 1\n\np_value = stats.distributions.t.cdf(t_value, dof)\n\nprint(\"The t value is {} and the p value is {}.\".format(t_value, p_value))\n\n","type":"content","url":"/tutorial-air-quality-analysis#calculating-the-test-statistics","position":27},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What do the t and p values mean?"},"type":"lvl2","url":"/tutorial-air-quality-analysis#what-do-the-t-and-p-values-mean","position":28},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"What do the t and p values mean?"},"content":"We will now compare the calculated test statistics with the critical test statistics. The critical t value is calculated by looking up the \n\nt-distribution table.\n\nFrom the table above, the critical value is 1.699 for 29 dof at a confidence level of 95%. Since we are using the left tailed test, our critical value is -1.699. Clearly, the calculated t value is less than the critical value so we can safely reject the null hypothesis.\n\nThe critical p value, denoted by \\alpha, is usually chosen to be 0.05, corresponding to a confidence level of 95%. If the calculated p value is less than \\alpha, then the null hypothesis can be safely rejected. Clearly, our p value is much less than \\alpha, so we can reject the null hypothesis.\n\nNote that this does not mean we can accept the alternative hypothesis. It only tells us that there is not enough evidence to reject H_{a}. In other words, we fail to reject the alternative hypothesis so, it may be true.\n\n","type":"content","url":"/tutorial-air-quality-analysis#what-do-the-t-and-p-values-mean","position":29},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"In practice..."},"type":"lvl2","url":"/tutorial-air-quality-analysis#in-practice","position":30},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"In practice..."},"content":"The \n\npandas library is preferable to use for time-series data analysis.\n\nThe SciPy stats module provides the \n\nstats.ttest_rel function which can be used to get the t statistic and p value.\n\nIn real life, data are generally not normally distributed. There are tests for such non-normal data like the \n\nWilcoxon test.","type":"content","url":"/tutorial-air-quality-analysis#in-practice","position":31},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Further reading"},"type":"lvl2","url":"/tutorial-air-quality-analysis#further-reading","position":32},{"hierarchy":{"lvl1":"Analyzing the impact of the lockdown on air quality in Delhi, India","lvl2":"Further reading"},"content":"There are a host of statistical tests you can choose according to the characteristics of the given data. Read more about them at\n\n\nA Gentle Introduction to Statistical Data Distributions.\n\nThere are various versions of the \n\nStudent’s t-test that you can adopt according to your needs.","type":"content","url":"/tutorial-air-quality-analysis#further-reading","position":33},{"hierarchy":{"lvl1":"Deep learning on MNIST"},"type":"lvl1","url":"/tutorial-deep-learning-on-mnist","position":0},{"hierarchy":{"lvl1":"Deep learning on MNIST"},"content":"This tutorial demonstrates how to build a simple \n\nfeedforward neural network (with one hidden layer) and train it from scratch with NumPy to recognize handwritten digit images.\n\nYour deep learning model — one of the most basic artificial neural networks that resembles the original \n\nmulti-layer perceptron — will learn to classify digits from 0 to 9 from the \n\nMNIST dataset. The dataset contains 60,000 training and 10,000 test images and corresponding labels. Each training and test image is of size 784 (or 28x28 pixels) — this will be your input for the neural network.\n\nBased on the image inputs and their labels (\n\nsupervised learning), your neural network will be trained to learn their features using forward propagation and backpropagation (\n\nreverse-mode differentiation). The final output of the network is a vector of 10 scores — one for each handwritten digit image. You will also evaluate how good your model is at classifying the images on the test set.\n\nThis tutorial was adapted from the work by \n\nAndrew Trask (with the author’s permission).","type":"content","url":"/tutorial-deep-learning-on-mnist","position":1},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"Prerequisites"},"type":"lvl2","url":"/tutorial-deep-learning-on-mnist#prerequisites","position":2},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"Prerequisites"},"content":"The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. In addition, you should be familiar with main concepts of \n\ndeep learning.\n\nTo refresh the memory, you can take the \n\nPython and \n\nLinear algebra on n-dimensional arrays tutorials.\n\nYou are advised to read the \n\nDeep learning paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. You should also consider reading Andrew Trask’s \n\nGrokking Deep Learning, which teaches deep learning with NumPy.\n\nIn addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing:\n\nurllib for URL handling\n\nrequest for URL opening\n\ngzip for gzip file decompression\n\npickle to work with the pickle file format\n\nas well as:\n\nMatplotlib for data visualization\n\nThis tutorial can be run locally in an isolated environment, such as \n\nVirtualenv or \n\nconda. You can use \n\nJupyter Notebook or JupyterLab to run each notebook cell. Don’t forget to \n\nset up NumPy and \n\nMatplotlib.","type":"content","url":"/tutorial-deep-learning-on-mnist#prerequisites","position":3},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"Table of contents"},"type":"lvl2","url":"/tutorial-deep-learning-on-mnist#table-of-contents","position":4},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"Table of contents"},"content":"Load the MNIST dataset\n\nPreprocess the dataset\n\nBuild and train a small neural network from scratch\n\nNext steps","type":"content","url":"/tutorial-deep-learning-on-mnist#table-of-contents","position":5},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"1. Load the MNIST dataset"},"type":"lvl2","url":"/tutorial-deep-learning-on-mnist#id-1-load-the-mnist-dataset","position":6},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"1. Load the MNIST dataset"},"content":"In this section, you will download the zipped MNIST dataset files originally developed by Yann LeCun’s research team. (More details of the MNIST dataset are available on \n\nKaggle.) Then, you will transform them into 4 files of NumPy array type using built-in Python modules. Finally, you will split the arrays into training and test sets.\n\n1. Define a variable to store the training/test image/label names of the MNIST dataset in a list:\n\ndata_sources = {\n    \"training_images\": \"train-images-idx3-ubyte.gz\",  # 60,000 training images.\n    \"test_images\": \"t10k-images-idx3-ubyte.gz\",  # 10,000 test images.\n    \"training_labels\": \"train-labels-idx1-ubyte.gz\",  # 60,000 training labels.\n    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",  # 10,000 test labels.\n}\n\n2. Load the data. First check if the data is stored locally; if not, then\ndownload it.\n\n# Use responsibly! When running notebooks locally, be sure to keep local\n# copies of the datasets to prevent unnecessary server requests\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/10.0\"\n}\nrequest_opts = {\n    \"headers\": headers,\n    \"params\": {\"raw\": \"true\"},\n}\n\nimport requests\nimport os\n\ndata_dir = \"../_data\"\nos.makedirs(data_dir, exist_ok=True)\n\nbase_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n\nfor fname in data_sources.values():\n    fpath = os.path.join(data_dir, fname)\n    if not os.path.exists(fpath):\n        print(\"Downloading file: \" + fname)\n        resp = requests.get(base_url + fname, stream=True, **request_opts)\n        resp.raise_for_status()  # Ensure download was succesful\n        with open(fpath, \"wb\") as fh:\n            for chunk in resp.iter_content(chunk_size=128):\n                fh.write(chunk)\n\n3. Decompress the 4 files and create 4 \n\nndarrays, saving them into a dictionary. Each original image is of size 28x28 and neural networks normally expect a 1D vector input; therefore, you also need to reshape the images by multiplying 28 by 28 (784).\n\nimport gzip\nimport numpy as np\n\nmnist_dataset = {}\n\n# Images\nfor key in (\"training_images\", \"test_images\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(\n            mnist_file.read(), np.uint8, offset=16\n        ).reshape(-1, 28 * 28)\n# Labels\nfor key in (\"training_labels\", \"test_labels\"):\n    with gzip.open(os.path.join(data_dir, data_sources[key]), \"rb\") as mnist_file:\n        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)\n\n4. Split the data into training and test sets using the standard notation of x for data and y for labels, calling the training and test set images x_train and x_test, and the labels y_train and y_test:\n\nx_train, y_train, x_test, y_test = (\n    mnist_dataset[\"training_images\"],\n    mnist_dataset[\"training_labels\"],\n    mnist_dataset[\"test_images\"],\n    mnist_dataset[\"test_labels\"],\n)\n\n5. You can confirm that the shape of the image arrays is (60000, 784) and (10000, 784) for training and test sets, respectively, and the labels — (60000,) and (10000,):\n\nprint(\n    \"The shape of training images: {} and training labels: {}\".format(\n        x_train.shape, y_train.shape\n    )\n)\nprint(\n    \"The shape of test images: {} and test labels: {}\".format(\n        x_test.shape, y_test.shape\n    )\n)\n\n6. And you can inspect some images using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Take the 60,000th image (indexed at 59,999) from the training set,\n# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\nmnist_image = x_train[59999, :].reshape(28, 28)\n# Set the color mapping to grayscale to have a black background.\nplt.imshow(mnist_image, cmap=\"gray\")\n# Display the image.\nplt.show()\n\n# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\nAbove are five images taken from the MNIST training set. Various hand-drawn\nArabic numerals are shown, with exact values chosen randomly with each run of the code.\n\nNote: You can also visualize a sample image as an array by printing x_train[59999]. Here, 59999 is your 60,000th training image sample (0 would be your first). Your output will be quite long and should contain an array of 8-bit integers:...\n         0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n...\n\n# Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]\n\n","type":"content","url":"/tutorial-deep-learning-on-mnist#id-1-load-the-mnist-dataset","position":7},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"2. Preprocess the data"},"type":"lvl2","url":"/tutorial-deep-learning-on-mnist#id-2-preprocess-the-data","position":8},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"2. Preprocess the data"},"content":"Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. When preprocessing the data, you should consider the following processes: \n\nvectorization and \n\nconversion to a floating-point format.\n\nSince the MNIST data is already vectorized and the arrays are of dtype uint8, your next challenge is to convert them to a floating-point format, such as float64 (\n\ndouble-precision):\n\nNormalizing the image data: a \n\nfeature scaling procedure that can speed up the neural network training process by standardizing the \n\ndistribution of your input data.\n\nOne-hot/categorical encoding of the image labels.\n\nIn practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the \n\nNvidia and \n\nGoogle Cloud blog posts.","type":"content","url":"/tutorial-deep-learning-on-mnist#id-2-preprocess-the-data","position":9},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Convert the image data to the floating-point format","lvl2":"2. Preprocess the data"},"type":"lvl3","url":"/tutorial-deep-learning-on-mnist#convert-the-image-data-to-the-floating-point-format","position":10},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Convert the image data to the floating-point format","lvl2":"2. Preprocess the data"},"content":"The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255.\n\nYou will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255.\n\n1. Check that the vectorized image data has type uint8:\n\nprint(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))\n\n2. Normalize the arrays by dividing them by 255 (and thus promoting the data type from uint8 to float64) and then assign the train and test image data variables — x_train and x_test — to training_images and train_labels, respectively.\nTo reduce the model training and evaluation time in this example, only a subset\nof the training and test images will be used.\nBoth training_images and test_images will contain only 1,000 samples each out\nof the complete datasets of 60,000 and 10,000 images, respectively.\nThese values can be controlled by changing the  training_sample and\ntest_sample below, up to their maximum values of 60,000 and 10,000.\n\ntraining_sample, test_sample = 1000, 1000\ntraining_images = x_train[0:training_sample] / 255\ntest_images = x_test[0:test_sample] / 255\n\n3. Confirm that the image data has changed to the floating-point format:\n\nprint(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))\n\nNote: You can also check that normalization was successful by printing training_images[0] in a notebook cell. Your long output should contain an array of floating-point numbers:...\n       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n...","type":"content","url":"/tutorial-deep-learning-on-mnist#convert-the-image-data-to-the-floating-point-format","position":11},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Convert the labels to floating point through categorical/one-hot encoding","lvl2":"2. Preprocess the data"},"type":"lvl3","url":"/tutorial-deep-learning-on-mnist#convert-the-labels-to-floating-point-through-categorical-one-hot-encoding","position":12},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Convert the labels to floating point through categorical/one-hot encoding","lvl2":"2. Preprocess the data"},"content":"You will use one-hot encoding to embed each digit label as an all-zero vector with np.zeros() and place 1 for a label index. As a result, your label data will be arrays with 1.0 (or 1.) in the position of each image label.\n\nSince there are 10 labels (from 0 to 9) in total, your arrays will look similar to this:array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n\n1. Confirm that the image label data are integers with dtype uint8:\n\nprint(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))\n\n2. Define a function that performs one-hot encoding on arrays:\n\ndef one_hot_encoding(labels, dimension=10):\n    # Define a one-hot variable for an all-zero vector\n    # with 10 dimensions (number labels from 0 to 9).\n    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n    # Return one-hot encoded labels.\n    return one_hot_labels.astype(np.float64)\n\n3. Encode the labels and assign the values to new variables:\n\ntraining_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])\n\n4. Check that the data type has changed to floating point:\n\nprint(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))\n\n5. Examine a few encoded labels:\n\nprint(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])\n\n...and compare to the originals:\n\nprint(y_train[0])\nprint(y_train[1])\nprint(y_train[2])\n\nYou have finished preparing the dataset.","type":"content","url":"/tutorial-deep-learning-on-mnist#convert-the-labels-to-floating-point-through-categorical-one-hot-encoding","position":13},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"3. Build and train a small neural network from scratch"},"type":"lvl2","url":"/tutorial-deep-learning-on-mnist#id-3-build-and-train-a-small-neural-network-from-scratch","position":14},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"3. Build and train a small neural network from scratch"},"content":"In this section you will familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the original \n\nDeep learning research publication for more information.\n\nAfterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy.","type":"content","url":"/tutorial-deep-learning-on-mnist#id-3-build-and-train-a-small-neural-network-from-scratch","position":15},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Neural network building blocks with NumPy","lvl2":"3. Build and train a small neural network from scratch"},"type":"lvl3","url":"/tutorial-deep-learning-on-mnist#neural-network-building-blocks-with-numpy","position":16},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Neural network building blocks with NumPy","lvl2":"3. Build and train a small neural network from scratch"},"content":"Layers: These building blocks work as data filters — they process data and learn representations from inputs to better predict the target outputs.\n\nYou will use 1 hidden layer in your model to pass the inputs forward (forward propagation) and propagate the gradients/error derivatives of a loss function backward (backpropagation). These are input, hidden and output layers.\n\nIn the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPy’s matrix multiplication function (the “dot multiply” or np.dot(layer, weights)).\n\nNote: For simplicity, the bias term is omitted in this example (there is no np.dot(layer, weights) + bias).\n\nWeights: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. They are optimized through a process called \n\ngradient descent. Before the model training starts, the weights are randomly initialized with NumPy’s \n\nGenerator.random().\n\nThe optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets.\n\nActivation function: Deep learning models are capable of determining non-linear relationships between inputs and outputs and these \n\nnon-linear functions are usually applied to the output of each layer.\n\nYou will use a \n\nrectified linear unit (ReLU) to the hidden layer’s output (for example, relu(np.dot(layer, weights)).\n\nRegularization: This \n\ntechnique helps prevent the neural network model from \n\noverfitting.\n\nIn this example, you will use a method called dropout — \n\ndilution — that randomly sets a number of features in a layer to 0s. You will define it with NumPy’s \n\nGenerator.integers() method and apply it to the hidden layer of the network.\n\nLoss function: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layer’s output.\n\nFor simplicity, you will use a basic total squared error using NumPy’s np.sum() function (for example, np.sum((final_layer_output - image_labels) ** 2)).\n\nAccuracy: This metric measures the accuracy of the network’s ability to predict on the data it hasn’t seen.","type":"content","url":"/tutorial-deep-learning-on-mnist#neural-network-building-blocks-with-numpy","position":17},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Model architecture and training summary","lvl2":"3. Build and train a small neural network from scratch"},"type":"lvl3","url":"/tutorial-deep-learning-on-mnist#model-architecture-and-training-summary","position":18},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Model architecture and training summary","lvl2":"3. Build and train a small neural network from scratch"},"content":"Here is a summary of the neural network model architecture and the training process:\n\nThe input layer:\n\nIt is the input for the network — the previously preprocessed data that is loaded from training_images into layer_0.\n\nThe hidden (middle) layer:\n\nlayer_1 takes the output from the previous layer and performs matrix-multiplication of the input by weights (weights_1) with NumPy’s np.dot()).\n\nThen, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting.\n\nThe output (last) layer:\n\nlayer_2 ingests the output from layer_1 and repeats the same “dot multiply” process with weights_2.\n\nThe final output returns 10 scores for each of the 0-9 digit labels. The network model ends with a size 10 layer — a 10-dimensional vector.\n\nForward propagation, backpropagation, training loop:\n\nIn the beginning of model training, your network randomly initializes the weights and feeds the input data forward through the hidden and output layers. This process is the forward pass or forward propagation.\n\nThen, the network propagates the “signal” from the loss function back through the hidden layer and adjusts the weights with the help of the learning rate parameter (more on that later).\n\nNote: In more technical terms, you:\n\nMeasure the error by comparing the real label of an image (the truth) with the prediction of the model.\n\nDifferentiate the loss function.\n\nIngest the \n\ngradients with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s).\n\nSince the network contains tensor operations and weight matrices, backpropagation uses the \n\nchain rule.\n\nWith each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics. As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model.","type":"content","url":"/tutorial-deep-learning-on-mnist#model-architecture-and-training-summary","position":19},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Compose the model and begin training and testing it","lvl2":"3. Build and train a small neural network from scratch"},"type":"lvl3","url":"/tutorial-deep-learning-on-mnist#compose-the-model-and-begin-training-and-testing-it","position":20},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl3":"Compose the model and begin training and testing it","lvl2":"3. Build and train a small neural network from scratch"},"content":"Having covered the main deep learning concepts and the neural network architecture, let’s write the code.\n\n1. We’ll start by creating a new random number generator, providing a seed\nfor reproducibility:\n\nseed = 884736743\nrng = np.random.default_rng(seed)\n\n2. For the hidden layer, define the ReLU activation function for forward propagation and ReLU’s derivative that will be used during backpropagation:\n\n# Define ReLU that returns the input if it's positive and 0 otherwise.\ndef relu(x):\n    return (x >= 0) * x\n\n\n# Set up a derivative of the ReLU function that returns 1 for a positive input\n# and 0 otherwise.\ndef relu2deriv(output):\n    return output >= 0\n\n3. Set certain default values of \n\nhyperparameters, such as:\n\nLearning rate: learning_rate — helps limit the magnitude of weight updates to prevent them from overcorrecting.\n\nEpochs (iterations): epochs — the number of complete passes — forward and backward propagations — of the data through the network. This parameter can positively or negatively affect the results. The higher the iterations, the longer the learning process may take. Because this is a computationally intensive task, we have chosen a very low number of epochs (20). To get meaningful results, you should choose a much larger number.\n\nSize of the hidden (middle) layer in a network: hidden_size — different sizes of the hidden layer can affect the results during training and testing.\n\nSize of the input: pixels_per_image — you have established that the image input is 784 (28x28) (in pixels).\n\nNumber of labels: num_labels — indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels.\n\nlearning_rate = 0.005\nepochs = 20\nhidden_size = 100\npixels_per_image = 784\nnum_labels = 10\n\n4. Initialize the weight vectors that will be used in the hidden and output layers with random values:\n\nweights_1 = 0.2 * rng.random((pixels_per_image, hidden_size)) - 0.1\nweights_2 = 0.2 * rng.random((hidden_size, num_labels)) - 0.1\n\n5. Set up the neural network’s learning experiment with a training loop and start the training process.\nNote that the model is evaluated against the test set at each epoch to track\nits performance over the training epochs.\n\nStart the training process:\n\n# To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )\n\nThe training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs. To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime (which will reset the weights), and run the notebook cells again.\n\nAfter executing the cell above, you can visualize the training and test set errors and accuracy for an instance of this training process.\n\nepoch_range = np.arange(epochs) + 1  # Starting from 1\n\n# The training set metrics.\ntraining_metrics = {\n    \"accuracy\": np.asarray(store_training_accurate_pred) / len(training_images),\n    \"error\": np.asarray(store_training_loss) / len(training_images),\n}\n\n# The test set metrics.\ntest_metrics = {\n    \"accuracy\": np.asarray(store_test_accurate_pred) / len(test_images),\n    \"error\": np.asarray(store_test_loss) / len(test_images),\n}\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nfor ax, metrics, title in zip(\n    axes, (training_metrics, test_metrics), (\"Training set\", \"Test set\")\n):\n    # Plot the metrics\n    for metric, values in metrics.items():\n        ax.plot(epoch_range, values, label=metric.capitalize())\n    ax.set_title(title)\n    ax.set_xlabel(\"Epochs\")\n    ax.legend()\nplt.show()\n\nThe training and testing error is shown above in the left and right\nplots, respectively. As the number of Epochs increases, the total error\ndecreases and the accuracy increases.\n\nThe accuracy rates that your model reaches during training and testing may be somewhat plausible but you may also find the error rates to be quite high.\n\nTo reduce the error during training and testing, you can consider changing the simple loss function to, for example, categorical \n\ncross-entropy. Other possible solutions are discussed below.","type":"content","url":"/tutorial-deep-learning-on-mnist#compose-the-model-and-begin-training-and-testing-it","position":21},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"Next steps"},"type":"lvl2","url":"/tutorial-deep-learning-on-mnist#next-steps","position":22},{"hierarchy":{"lvl1":"Deep learning on MNIST","lvl2":"Next steps"},"content":"You have learned how to build and train a simple feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits.\n\nTo further enhance and optimize your neural network model, you can consider one of a mixture of the following:\n\nIncrease the training sample size from 1,000 to a higher number (up to 60,000).\n\nUse \n\nmini-batches and reduce the learning rate.\n\nAlter the architecture by introducing more hidden layers to make the network \n\ndeeper.\n\nCombine the \n\ncross-entropy loss function with a \n\nsoftmax activation function in the last layer.\n\nIntroduce convolutional layers: replace the feedforward network with a \n\nconvolutional neural network architecture.\n\nUse a higher epoch size to train longer and add more regularization techniques, such as \n\nearly stopping, to prevent \n\noverfitting.\n\nIntroduce a \n\nvalidation set for an unbiased valuation of the model fit.\n\nApply \n\nbatch normalization for faster and more stable training.\n\nTune other parameters, such as the learning rate and hidden layer size.\n\nBuilding a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks — such as \n\nPyTorch, \n\nJAX, \n\nTensorFlow or \n\nMXNet — that provide NumPy-like APIs, have built-in \n\nautomatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning.\n\nFinally, when developing a machine learning model, you should think about potential ethical issues and apply practices to avoid or mitigate those:\n\nDocument a trained model with a Model Card - see the \n\nModel Cards for Model Reporting paper by Margaret Mitchell et al..\n\nDocument a dataset with a Datasheet - see the \n\nDatasheets for Datasets paper) by Timnit Gebru et al..\n\nConsider the impact of your model - who is affected by it, who does it benefit - see \n\nthe article and \n\ntalk by Pratyusha Kalluri.\n\nFor more resources, see \n\nthis blog post by Rachel Thomas and the \n\nRadical AI podcast.\n\n(Credit to \n\nhsjeong5 for demonstrating how to download MNIST without the use of external libraries.)","type":"content","url":"/tutorial-deep-learning-on-mnist#next-steps","position":23},{"hierarchy":{"lvl1":"Masked Arrays"},"type":"lvl1","url":"/tutorial-ma","position":0},{"hierarchy":{"lvl1":"Masked Arrays"},"content":"","type":"content","url":"/tutorial-ma","position":1},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What you’ll do"},"type":"lvl2","url":"/tutorial-ma#what-youll-do","position":2},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What you’ll do"},"content":"Use the masked arrays module from NumPy to analyze COVID-19 data and deal with missing values.","type":"content","url":"/tutorial-ma#what-youll-do","position":3},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What you’ll learn"},"type":"lvl2","url":"/tutorial-ma#what-youll-learn","position":4},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What you’ll learn"},"content":"You’ll understand what are masked arrays and how they can be created\n\nYou’ll see how to access and modify data for masked arrays\n\nYou’ll be able to decide when the use of masked arrays is appropriate in some of your applications","type":"content","url":"/tutorial-ma#what-youll-learn","position":5},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What you’ll need"},"type":"lvl2","url":"/tutorial-ma#what-youll-need","position":6},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What you’ll need"},"content":"Basic familiarity with Python. If you would like to refresh your memory, take a look at the \n\nPython tutorial.\n\nBasic familiarity with NumPy\n\nTo run the plots on your computer, you need \n\nmatplotlib.\n\n\n\n","type":"content","url":"/tutorial-ma#what-youll-need","position":7},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What are masked arrays?"},"type":"lvl2","url":"/tutorial-ma#what-are-masked-arrays","position":8},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"What are masked arrays?"},"content":"Consider the following problem. You have a dataset with missing or invalid entries. If you’re doing any kind of processing on this data, and want to skip or flag these unwanted entries without just deleting them, you may have to use conditionals or filter your data somehow. The \n\nnumpy.ma module provides some of the same functionality of \n\nNumPy ndarrays with added structure to ensure invalid entries are not used in computation.\n\nFrom the \n\nReference Guide:\n\nA masked array is the combination of a standard \n\nnumpy.ndarray and a mask. A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid).\n\nWe can think of a \n\nMaskedArray as a combination of:\n\nData, as a regular numpy.ndarray of any shape or datatype;\n\nA boolean mask with the same shape as the data;\n\nA fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy.ndarray.","type":"content","url":"/tutorial-ma#what-are-masked-arrays","position":9},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"When can they be useful?"},"type":"lvl2","url":"/tutorial-ma#when-can-they-be-useful","position":10},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"When can they be useful?"},"content":"There are a few situations where masked arrays can be more useful than just eliminating the invalid entries of an array:\n\nWhen you want to preserve the values you masked for later processing, without copying the array;\n\nWhen you have to handle many arrays, each with their own mask. If the mask is part of the array, you avoid bugs and the code is possibly more compact;\n\nWhen you have different flags for missing or invalid values, and wish to preserve these flags without replacing them in the original dataset, but exclude them from computations;\n\nIf you can’t avoid or eliminate missing values, but don’t want to deal with \n\nNaN (Not a Number) values in your operations.\n\nMasked arrays are also a good idea since the numpy.ma module also comes with a specific implementation of most \n\nNumPy universal functions (ufuncs), which means that you can still apply fast vectorized functions and operations on masked data. The output is then a masked array. We’ll see some examples of how this works in practice below.\n\n","type":"content","url":"/tutorial-ma#when-can-they-be-useful","position":11},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Using masked arrays to see COVID-19 data"},"type":"lvl2","url":"/tutorial-ma#using-masked-arrays-to-see-covid-19-data","position":12},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Using masked arrays to see COVID-19 data"},"content":"From \n\nKaggle it is possible to download a dataset with initial data about the COVID-19 outbreak in the beginning of 2020. We are going to look at a small subset of this data, contained in the file who_covid_19_sit_rep_time_series.csv. (Note that this file has been replaced with a version without missing data sometime in late 2020.)\n\nimport numpy as np\nimport os\n\n# The os.getcwd() function returns the current folder; you can change\n# the filepath variable to point to the folder where you saved the .csv file\nfilepath = os.getcwd()\nfilename = os.path.join(filepath, \"who_covid_19_sit_rep_time_series.csv\")\n\nThe data file contains data of different types and is organized as follows:\n\nThe first row is a header line that (mostly) describes the data in each column that follow in the rows below, and beginning in the fourth column, the header is the date of the observation.\n\nThe second through seventh row contain summary data that is of a different type than that which we are going to examine, so we will need to exclude that from the data with which we will work.\n\nThe numerical data we wish to work with begins at column 4, row 8, and extends from there to the rightmost column and the lowermost row.\n\nLet’s explore the data inside this file for the first 14 days of records. To gather data from the .csv file, we will use the \n\nnumpy.genfromtxt function, making sure we select only the columns with actual numbers instead of the first four columns which contain location data. We also skip the first 6\nrows of this file, since they contain other data we are not interested in. Separately, we will extract the information about dates and location for this data.\n\n# Note we are using skip_header and usecols to read only portions of the\n# data file into each variable.\n# Read just the dates for columns 4-18 from the first row\ndates = np.genfromtxt(\n    filename,\n    dtype=np.str_,\n    delimiter=\",\",\n    max_rows=1,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)\n# Read the names of the geographic locations from the first two\n# columns, skipping the first six rows\nlocations = np.genfromtxt(\n    filename,\n    dtype=np.str_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=(0, 1),\n    encoding=\"utf-8-sig\",\n)\n# Read the numeric data from just the first 14 days\nnbcases = np.genfromtxt(\n    filename,\n    dtype=np.int_,\n    delimiter=\",\",\n    skip_header=6,\n    usecols=range(4, 18),\n    encoding=\"utf-8-sig\",\n)\n\nIncluded in the numpy.genfromtxt function call, we have selected the \n\nnumpy.dtype for each subset of the data (either an integer - numpy.int_ - or a string of characters - numpy.str_). We have also used the encoding argument to select utf-8-sig as the encoding for the file (read more about encoding in the \n\nofficial Python documentation. You can read more about the numpy.genfromtxt function from the \n\nReference Documentation or from the \n\nBasic IO tutorial.\n\n","type":"content","url":"/tutorial-ma#using-masked-arrays-to-see-covid-19-data","position":13},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Exploring the data"},"type":"lvl2","url":"/tutorial-ma#exploring-the-data","position":14},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Exploring the data"},"content":"First of all, we can plot the whole set of data we have and see what it looks like. In order to get a readable plot, we select only a few of the dates to show in our \n\nx-axis ticks. Note also that in our plot command, we use nbcases.T (the transpose of the nbcases array) since this means we will plot each row of the file as a separate line. We choose to plot a dashed line (using the '--' line style). See the \n\nmatplotlib documentation for more info on this.\n\nimport matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")\n\nThe graph has a strange shape from January 24th to February 1st. It would be interesting to know where this data comes from. If we look at the locations array we extracted from the .csv file, we can see that we have two columns, where the first would contain regions and the second would contain the name of the country. However, only the first few rows contain data for the the first column (province names in China). Following that, we only have country names. So it would make sense to group all the data from China into a single row. For this, we’ll select from the nbcases array only the rows for which the second entry of the locations array corresponds to China. Next, we’ll use the \n\nnumpy.sum function to sum all the selected rows (axis=0). Note also that row 35 corresponds to the total counts for the whole country for each date. Since we want to calculate the sum ourselves from the provinces data, we have to remove that row first from both locations and nbcases:\n\ntotals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total\n\nSomething’s wrong with this data - we are not supposed to have negative values in a cumulative data set. What’s going on?\n\n","type":"content","url":"/tutorial-ma#exploring-the-data","position":15},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Missing data"},"type":"lvl2","url":"/tutorial-ma#missing-data","position":16},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Missing data"},"content":"Looking at the data, here’s what we find: there is a period with missing data:\n\nnbcases\n\nAll the -1 values we are seeing come from numpy.genfromtxt attempting to read missing data from the original .csv file. Obviously, we\ndon’t want to compute missing data as -1 - we just want to skip this value so it doesn’t interfere in our analysis. After importing the numpy.ma module, we’ll create a new array, this time masking the invalid values:\n\nfrom numpy import ma\n\nnbcases_ma = ma.masked_values(nbcases, -1)\n\nIf we look at the nbcases_ma masked array, this is what we have:\n\nnbcases_ma\n\nWe can see that this is a different kind of array. As mentioned in the introduction, it has three attributes (data, mask and fill_value).\nKeep in mind that the mask attribute has a True value for elements corresponding to invalid data (represented by two dashes in the data attribute).\n\nLet’s try and see what the data looks like excluding the first row (data from the Hubei province in China) so we can look at the missing data more\nclosely:\n\nplt.plot(dates, nbcases_ma[1:].T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")\n\nNow that our data has been masked, let’s try summing up all the cases in China:\n\nchina_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked\n\nNote that china_masked is a masked array, so it has a different data structure than a regular NumPy array. Now, we can access its data directly by using the .data attribute:\n\nchina_total = china_masked.data\nchina_total\n\nThat is better: no more negative values. However, we can still see that for some days, the cumulative number of cases seems to go down (from 835 to 10, for example), which does not agree with the definition of “cumulative data”. If we look more closely at the data, we can see that in the period where there was missing data in mainland China, there was valid data for Hong Kong, Taiwan, Macau and “Unspecified” regions of China. Maybe we can remove those from the total sum of cases in China, to get a better understanding of the data.\n\nFirst, we’ll identify the indices of locations in mainland China:\n\nchina_mask = (\n    (locations[:, 1] == \"China\")\n    & (locations[:, 0] != \"Hong Kong\")\n    & (locations[:, 0] != \"Taiwan\")\n    & (locations[:, 0] != \"Macau\")\n    & (locations[:, 0] != \"Unspecified*\")\n)\n\nNow, china_mask is an array of boolean values (True or False); we can check that the indices are what we wanted with the \n\nma.nonzero method for masked arrays:\n\nchina_mask.nonzero()\n\nNow we can correctly sum entries for mainland China:\n\nchina_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total\n\nWe can replace the data with this information and plot a new graph, focusing on Mainland China:\n\nplt.plot(dates, china_total.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\")\n\nIt’s clear that masked arrays are the right solution here. We cannot represent the missing data without mischaracterizing the evolution of the curve.\n\n","type":"content","url":"/tutorial-ma#missing-data","position":17},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Fitting Data"},"type":"lvl2","url":"/tutorial-ma#fitting-data","position":18},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Fitting Data"},"content":"One possibility we can think of is to interpolate the missing data to estimate the number of cases in late January. Observe that we can select the masked elements using the .mask attribute:\n\nchina_total.mask\ninvalid = china_total[china_total.mask]\ninvalid\n\nWe can also access the valid entries by using the logical negation for this mask:\n\nvalid = china_total[~china_total.mask]\nvalid\n\nNow, if we want to create a very simple approximation for this data, we should take into account the valid entries around the invalid ones. So first let’s select the dates for which the data is valid. Note that we can use the mask from the china_total masked array to index the dates array:\n\ndates[~china_total.mask]\n\nFinally, we can use the\n\n\nfitting functionality of the numpy.polynomial\npackage to create a cubic polynomial model that fits the data as best as possible:\n\nt = np.arange(len(china_total))\nmodel = np.polynomial.Polynomial.fit(t[~china_total.mask], valid, deg=3)\nplt.plot(t, china_total)\nplt.plot(t, model(t), \"--\")\n\nThis plot is not so readable since the lines seem to be over each other, so let’s summarize in a more elaborate plot. We’ll plot the real data when\navailable, and show the cubic fit for unavailable data, using this fit to compute an estimate to the observed number of cases on January 28th 2020, 7 days after the beginning of the records:\n\nplt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)\n\n","type":"content","url":"/tutorial-ma#fitting-data","position":19},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"In practice"},"type":"lvl2","url":"/tutorial-ma#in-practice","position":20},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"In practice"},"content":"\n\nAdding -1 to missing data is not a problem with numpy.genfromtxt; in this particular case, substituting the missing value with 0 might have been fine, but we’ll see later that this is far from a general solution. Also, it is possible to call the numpy.genfromtxt function using the usemask parameter. If usemask=True, numpy.genfromtxt automatically returns a masked array.\n\n","type":"content","url":"/tutorial-ma#in-practice","position":21},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Further reading"},"type":"lvl2","url":"/tutorial-ma#further-reading","position":22},{"hierarchy":{"lvl1":"Masked Arrays","lvl2":"Further reading"},"content":"Topics not covered in this tutorial can be found in the documentation:\n\nHardmasks vs. \n\nsoftmasks\n\nThe numpy.ma module","type":"content","url":"/tutorial-ma#further-reading","position":23},{"hierarchy":{"lvl1":"Masked Arrays","lvl3":"Reference","lvl2":"Further reading"},"type":"lvl3","url":"/tutorial-ma#reference","position":24},{"hierarchy":{"lvl1":"Masked Arrays","lvl3":"Reference","lvl2":"Further reading"},"content":"Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, \n\nDong et al. (2020).","type":"content","url":"/tutorial-ma#reference","position":25},{"hierarchy":{"lvl1":"Plotting Fractals"},"type":"lvl1","url":"/tutorial-plotting-fractals","position":0},{"hierarchy":{"lvl1":"Plotting Fractals"},"content":"\n\nFractals are beautiful, compelling mathematical forms that can be oftentimes created from a relatively simple set of instructions. In nature they can be found in various places, such as coastlines, seashells, and ferns, and even were used in creating certain types of antennas. The mathematical idea of fractals was known for quite some time, but they really began to be truly appreciated in the 1970’s as advancements in computer graphics and some accidental discoveries lead researchers like \n\nBenoît Mandelbrot to stumble upon the truly mystifying visualizations that fractals possess.\n\nToday we will learn how to plot these beautiful visualizations and will start to do a bit of exploring for ourselves as we gain familiarity of the mathematics behind fractals and will use the ever powerful NumPy universal functions to perform the necessary calculations efficiently.\n\n","type":"content","url":"/tutorial-plotting-fractals","position":1},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"What you’ll do"},"type":"lvl2","url":"/tutorial-plotting-fractals#what-youll-do","position":2},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"What you’ll do"},"content":"Write a function for plotting various Julia sets\n\nCreate a visualization of the Mandelbrot set\n\nWrite a function that computes Newton fractals\n\nExperiment with variations of general fractal types\n\n","type":"content","url":"/tutorial-plotting-fractals#what-youll-do","position":3},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"What you’ll learn"},"type":"lvl2","url":"/tutorial-plotting-fractals#what-youll-learn","position":4},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"What you’ll learn"},"content":"A better intuition for how fractals work mathematically\n\nA basic understanding about NumPy universal functions and Boolean Indexing\n\nThe basics of working with complex numbers in NumPy\n\nHow to create your own unique fractal visualizations\n\n","type":"content","url":"/tutorial-plotting-fractals#what-youll-learn","position":5},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"What you’ll need"},"type":"lvl2","url":"/tutorial-plotting-fractals#what-youll-need","position":6},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"What you’ll need"},"content":"Matplotlib\n\nmake_axis_locatable function from mpl_toolkits API\n\nwhich can be imported as follows:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nSome familiarity with Python, NumPy and matplotlib\n\nAn idea of elementary mathematical functions, such as \n\nexponents, \n\nsin, \n\npolynomials etc\n\nA very basic understanding of \n\ncomplex numbers would be useful\n\nKnowledge of \n\nderivatives may be helpful\n\n","type":"content","url":"/tutorial-plotting-fractals#what-youll-need","position":7},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Warmup"},"type":"lvl2","url":"/tutorial-plotting-fractals#warmup","position":8},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Warmup"},"content":"To gain some intuition for what fractals are, we will begin with an example.\n\nConsider the following equation:\n\nf(z) = z^2 -1 \n\nwhere z is a complex number (i.e of the form a + bi )\n\nFor our convenience, we will write a Python function for it\n\ndef f(z):\n    return np.square(z) - 1\n\nNote that the square function we used is an example of a NumPy universal function; we will come back to the significance of this decision shortly.\n\nTo gain some intuition for the behaviour of the function, we can try plugging in some different values.\n\nFor z = 0, we would expect to get -1:\n\nf(0)\n\nSince we used a universal function in our design, we can compute multiple inputs at the same time:\n\nz = [4, 1-0.2j, 1.6]\nf(z)\n\nSome values grow, some values shrink, some don’t experience much change.\n\nTo see the behaviour of the function on a larger scale, we can apply the function to a subset of the complex plane and plot the result. To create our subset (or mesh), we can make use of the \n\nmeshgrid function.\n\nx, y = np.meshgrid(np.linspace(-10, 10, 20), np.linspace(-10, 10, 20))\nmesh = x + (1j * y)  # Make mesh of complex plane\n\nNow we will apply our function to each value contained in the mesh. Since we used a universal function in our design, this means that we can pass in the entire mesh all at once. This is extremely convenient for two reasons: It reduces the amount of code needed to be written and greatly increases the efficiency (as universal functions make use of system level C programming in their computations).\n\nHere we plot the absolute value (or modulus) of each element in the mesh after one “iteration” of the function using a \n\n3D scatterplot:\n\noutput = np.abs(f(mesh))  # Take the absolute value of the output (for plotting)\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\n\nax.scatter(x, y, output, alpha=0.2)\n\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Absolute value')\nax.set_title('One Iteration: $ f(z) = z^2 - 1$');\n\nThis gives us a rough idea of what one iteration of the function does. Certain areas (notably in the areas closest to (0,0i)) remain rather small while other areas grow quite considerably. Note that we lose information about the output by taking the absolute value, but it is the only way for us to be able to make a plot.\n\nLet’s see what happens when we apply 2 iterations to the mesh:\n\noutput = np.abs(f(f(mesh)))\n\nax = plt.axes(projection='3d')\n\nax.scatter(x, y, output, alpha=0.2)\n\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Absolute value')\nax.set_title('Two Iterations: $ f(z) = z^2 - 1$');\n\nOnce again, we see that values around the origin remain small, and values with a larger absolute value (or modulus) “explode”.\n\nFrom first impression, its behaviour appears to be normal, and may even seem mundane. Fractals tend to have more to them then what meets the eye; the exotic behavior shows itself when we begin applying more iterations.\n\nConsider three complex numbers:\n\nz_1 = 0.4 + 0.4i ,\n\nz_2 = z_1 + 0.1,\n\nz_3 = z_1 + 0.1i\n\nGiven the shape of our first two plots, we would expect that these values would remain near the origin as we apply iterations to them. Let us see what happens when we apply 10 iterations to each value:\n\nselected_values = np.array([0.4 + 0.4j, 0.41 + 0.4j, 0.4 + 0.41j])\nnum_iter = 9\n\noutputs = np.zeros((num_iter+1, selected_values.shape[0]), dtype=complex)\noutputs[0] = selected_values\n\nfor i in range(num_iter):\n    outputs[i+1] = f(outputs[i])  # Apply 10 iterations, save each output\n\nfig, axes = plt.subplots(1, selected_values.shape[0], figsize=(16, 6))\naxes[1].set_xlabel('Real axis')\naxes[0].set_ylabel('Imaginary axis')\n\nfor ax, data in zip(axes, outputs.T):\n    cycle = ax.scatter(data.real, data.imag, c=range(data.shape[0]), alpha=0.6)\n    ax.set_title(f'Mapping of iterations on {data[0]}')\n\nfig.colorbar(cycle, ax=axes, location=\"bottom\", label='Iteration');\n\nTo our surprise, the behaviour of the function did not come close to matching our hypothesis. This is a prime example of the chaotic behaviour fractals possess. In the first two plots, the value “exploded” on the last iteration, jumping way beyond the region that it was contained in previously. The third plot on the other hand remained bounded to a small region close to the origin, yielding completely different behaviour despite the tiny change in value.\n\nThis leads us to an extremely important question: How many iterations can be applied to each value before they diverge (“explode”)?\n\nAs we saw from the first two plots, the further the values were from the origin, the faster they generally exploded. Although the behaviour is uncertain for smaller values (like z_1, z_2, z_3), we can assume that if a value surpasses a certain distance from the origin (say 2) that it is doomed to diverge. We will call this threshold the radius.\n\nThis allows us to quantify the behaviour of the function for a particular value without having to perform as many computations. Once the radius is surpassed, we are allowed to stop iterating, which gives us a way of answering the question we posed. If we tally how many computations were applied before divergence, we gain insight into the behaviour of the function that would be hard to keep track of otherwise.\n\nOf course, we can do much better and design a function that performs the procedure on an entire mesh.\n\ndef divergence_rate(mesh, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(mesh.shape)  # Keep tally of the number of iterations\n\n    # Iterate on element if and only if |element| < radius (Otherwise assume divergence)\n    for i in range(num_iter):\n        conv_mask = np.abs(z) < radius\n        diverge_len[conv_mask] += 1\n        z[conv_mask] = f(z[conv_mask])\n\n    return diverge_len\n\nThe behaviour of this function may look confusing at first glance, so it will help to explain some of the notation.\n\nOur goal is to iterate over each value in the mesh and to tally the number of iterations before the value diverges. Since some values will diverge quicker than others, we need a procedure that only iterates over values that have an absolute value that is sufficiently small enough. We also want to stop tallying values once they surpass the radius. For this, we can use Boolean Indexing, a NumPy feature that when paired with universal functions is unbeatable. Boolean Indexing allows for operations to be performed conditionally on a NumPy array without having to resort to looping over and checking for each array value individually.\n\nIn our case, we use a loop to apply iterations to our function f(z) = z^2 -1  and keep tally. Using Boolean indexing, we only apply the iterations to values that have an absolute value less than 2.\n\nWith that out of the way, we can go about plotting our first fractal! We will use the \n\nimshow function to create a colour-coded visualization of the tallies.\n\nx, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)\n\noutput = divergence_rate(mesh)\n\nfig = plt.figure(figsize=(5, 5))\nax = plt.axes()\n\nax.set_title('$f(z) = z^2 -1$')\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\n\nim = ax.imshow(output, extent=[-2, 2, -2, 2])\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\nplt.colorbar(im, cax=cax, label='Number of iterations');\n\nWhat this stunning visual conveys is the complexity of the function’s behaviour. The yellow region represents values that remain small, while the purple region represents the divergent values. The beautiful pattern that arises on the border of the converging and diverging values is even more fascinating when you realize that it is created from such a simple function.\n\n","type":"content","url":"/tutorial-plotting-fractals#warmup","position":9},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Julia set"},"type":"lvl2","url":"/tutorial-plotting-fractals#julia-set","position":10},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Julia set"},"content":"What we just explored was an example of a fractal visualization of a specific Julia Set.\n\nConsider the function f(z) = z^2 + c where c is a complex number. The filled-in Julia set of c is the set of all complex numbers z in which the function converges at f(z). Likewise, the boundary of the filled-in Julia set is what we call the Julia set. In our above visualization, we can see that the yellow region represents an approximation of the filled-in Julia set for c = -1 and the greenish-yellow border would contain the Julia set.\n\nTo gain access to a wider range of “Julia fractals”, we can write a function that allows for different values of c to be passed in:\n\ndef julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) < radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len\n\nTo make our lives easier, we will create a couple meshes that we will reuse throughout the rest of the examples:\n\nx, y = np.meshgrid(np.linspace(-1, 1, 400), np.linspace(-1, 1, 400))\nsmall_mesh = x + (1j * y)\n\nx, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)\n\nWe will also write a function that we will use to create our fractal plots:\n\ndef plot_fractal(fractal, title='Fractal', figsize=(6, 6), cmap='rainbow', extent=[-2, 2, -2, 2]):\n\n    plt.figure(figsize=figsize)\n    ax = plt.axes()\n\n    ax.set_title(f'${title}$')\n    ax.set_xlabel('Real axis')\n    ax.set_ylabel('Imaginary axis')\n\n    im = ax.imshow(fractal, extent=extent, cmap=cmap)\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    plt.colorbar(im, cax=cax, label='Number of iterations')\n\nUsing our newly defined functions, we can make a quick plot of the first fractal again:\n\noutput = julia(mesh, num_iter=15)\nkwargs = {'title': 'f(z) = z^2 -1'}\n\nplot_fractal(output, **kwargs);\n\nWe also can explore some different Julia sets by experimenting with different values of c. It can be surprising how much influence it has on the shape of the fractal.\n\nFor example, setting c = \\frac{\\pi}{10} gives us a very elegant cloud shape, while setting c = -\\frac{3}{4} + 0.4i yields a completely different pattern.\n\noutput = julia(mesh, c=np.pi/10, num_iter=20)\nkwargs = {'title': r'f(z) = z^2 + \\dfrac{\\pi}{10}', 'cmap': 'plasma'}\n\nplot_fractal(output, **kwargs);\n\n\n\noutput = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': r'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n","type":"content","url":"/tutorial-plotting-fractals#julia-set","position":11},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Mandelbrot set"},"type":"lvl2","url":"/tutorial-plotting-fractals#mandelbrot-set","position":12},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Mandelbrot set"},"content":"Closely related to the Julia set is the famous Mandelbrot set, which has a slightly different definition. Once again, we define f(z) = z^2 + c where c is a complex number, but this time our focus is on our choice of c. We say that c is an element of the Mandelbrot set if f converges at z = 0. An equivalent definition is to say that c is an element of the Mandelbrot set if f(c) can be iterated infinitely and not ‘explode’. We will tweak our Julia function slightly (and rename it appropriately) so that we can plot a visualization of the Mandelbrot set, which possesses an elegant fractal pattern.\n\ndef mandelbrot(mesh, num_iter=10, radius=2):\n\n    c = mesh.copy()\n    z = np.zeros(mesh.shape, dtype=np.complex128)\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) < radius\n        z[conv_mask] = np.square(z[conv_mask]) + c[conv_mask]\n        diverge_len[conv_mask] += 1\n\n    return diverge_len\n\n\n\noutput = mandelbrot(mesh, num_iter=50)\nkwargs = {'title': 'Mandelbrot \\\\ set', 'cmap': 'hot'}\n\nplot_fractal(output, **kwargs);\n\n","type":"content","url":"/tutorial-plotting-fractals#mandelbrot-set","position":13},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Generalizing the Julia set"},"type":"lvl2","url":"/tutorial-plotting-fractals#generalizing-the-julia-set","position":14},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Generalizing the Julia set"},"content":"We can generalize our Julia function even further by giving it a parameter for which universal function we would like to pass in. This would allow us to plot fractals of the form f(z) = g(z) + c where g is a universal function selected by us.\n\ndef general_julia(mesh, c=-1, f=np.square, num_iter=100, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) < radius\n        z[conv_mask] = f(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len\n\nOne cool set of fractals that can be plotted using our general Julia function are ones of the form f(z) = z^n + c for some positive integer n. A very cool pattern which emerges is that the number of regions that ‘stick out’ matches the degree in which we raise the function to while iterating over it.\n\nfig, axes = plt.subplots(2, 3, figsize=(8, 8))\nbase_degree = 2\n\nfor deg, ax in enumerate(axes.ravel()):\n    degree = base_degree + deg\n    power = lambda z: np.power(z, degree)  # Create power function for current degree\n\n    diverge_len = general_julia(mesh, f=power, num_iter=15)\n    ax.imshow(diverge_len, extent=[-2, 2, -2, 2], cmap='binary')\n    ax.set_title(f'$f(z) = z^{degree} -1$')\n\nNeedless to say, there is a large amount of exploring that can be done by fiddling with the inputted function, value of c, number of iterations, radius and even the density of the mesh and choice of colours.\n\n","type":"content","url":"/tutorial-plotting-fractals#generalizing-the-julia-set","position":15},{"hierarchy":{"lvl1":"Plotting Fractals","lvl3":"Newton Fractals","lvl2":"Generalizing the Julia set"},"type":"lvl3","url":"/tutorial-plotting-fractals#newton-fractals","position":16},{"hierarchy":{"lvl1":"Plotting Fractals","lvl3":"Newton Fractals","lvl2":"Generalizing the Julia set"},"content":"Newton fractals are a specific class of fractals, where iterations involve adding or subtracting the ratio of a function (often a polynomial) and its derivative to the input values. Mathematically, it can be expressed as:\n\nz := z - \\frac{f(z)}{f'(z)}\n\nWe will define a general version of the fractal which will allow for different variations to be plotted by passing in our functions of choice.\n\ndef newton_fractal(mesh, f, df, num_iter=10, r=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) < r\n        pz = f(z[conv_mask])\n        dp = df(z[conv_mask])\n        z[conv_mask] = z[conv_mask] - pz/dp\n        diverge_len[conv_mask] += 1\n\n    return diverge_len\n\nNow we can experiment with some different functions. For polynomials, we can create our plots quite effortlessly using the \n\nNumPy Polynomial class, which has built in functionality for computing derivatives.\n\nFor example, let’s try a higher-degree polynomial:\n\np = np.polynomial.Polynomial([-16, 0, 0, 0, 15, 0, 0, 0, 1])\np\n\nwhich has the derivative:\n\np.deriv()\n\n\n\noutput = newton_fractal(mesh, p, p.deriv(), num_iter=15, r=2)\nkwargs = {'title': r'f(z) = z - \\dfrac{(z^8 + 15z^4 - 16)}{(8z^7 + 60z^3)}', 'cmap': 'copper'}\n\nplot_fractal(output, **kwargs)\n\nBeautiful! Let’s try another one:\n\nf(z) = tan^2(z)\n\n\\frac{df}{dz} = 2 \\cdot tan(z) sec^2(z) =\\frac{2 \\cdot tan(z)}{cos^2(z)}\n\nThis makes \\frac{f(z)}{f'(z)} =  tan^2(z) \\cdot \\frac{cos^2(z)}{2 \\cdot tan(z)} = \\frac{tan(z)\\cdot cos^2(z)}{2} = \\frac{sin(z)\\cdot cos(z)}{2}\n\ndef f_tan(z):\n    return np.square(np.tan(z))\n\n\ndef d_tan(z):\n    return 2*np.tan(z) / np.square(np.cos(z))\n\n\n\noutput = newton_fractal(mesh, f_tan, d_tan, num_iter=15, r=50)\nkwargs = {'title': r'f(z) = z - \\dfrac{sin(z)cos(z)}{2}', 'cmap': 'binary'}\n\nplot_fractal(output, **kwargs);\n\nNote that you sometimes have to play with the radius in order to get a neat looking fractal.\n\nFinally, we can go a little bit wild with our function selection\n\nf(z) = \\sum_{i=1}^{10} sin^i(z)\n\n\\frac{df}{dz} = \\sum_{i=1}^{10} i \\cdot sin^{i-1}(z) \\cdot cos(z)\n\ndef sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += np.power(np.sin(z), i)\n    return total\n\n\ndef d_sin_sum(z, n=10):\n    total = np.zeros(z.size, dtype=z.dtype)\n    for i in range(1, n+1):\n        total += i * np.power(np.sin(z), i-1) * np.cos(z)\n    return total\n\nWe will denote this one ‘Wacky fractal’, as its equation would not be fun to try and put in the title.\n\noutput = newton_fractal(small_mesh, sin_sum, d_sin_sum, num_iter=10, r=1)\nkwargs = {'title': 'Wacky \\\\ fractal', 'figsize': (6, 6), 'extent': [-1, 1, -1, 1], 'cmap': 'terrain'}\n\nplot_fractal(output, **kwargs)\n\nIt is truly fascinating how distinct yet similar these fractals are with each other. This leads us to the final section.\n\n","type":"content","url":"/tutorial-plotting-fractals#newton-fractals","position":17},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Creating your own fractals"},"type":"lvl2","url":"/tutorial-plotting-fractals#creating-your-own-fractals","position":18},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Creating your own fractals"},"content":"What makes fractals more exciting is how much there is to explore once you become familiar with the basics. Now we will wrap up our tutorial by exploring some of the different ways one can experiment in creating unique fractals. I encourage you to try some things out on your own (if you have not done so already).\n\nOne of the first places to experiment would be with the function for the generalized Julia set, where we can try passing in different functions as parameters.\n\nLet’s start by choosing\n\nf(z) = tan(z^2)\n\ndef f(z):\n    return np.tan(np.square(z))\n\n\n\noutput = general_julia(mesh, f=f, num_iter=15, radius=2.1)\nkwargs = {'title': 'f(z) = tan(z^2)', 'cmap': 'gist_stern'}\n\nplot_fractal(output, **kwargs);\n\nWhat happens if we compose our defined function inside of a sine function?\n\nLet’s try defining\n\ng(z) = sin(f(z)) = sin(tan(z^2))\n\ndef g(z):\n    return np.sin(f(z))\n\n\n\noutput = general_julia(mesh, f=g, num_iter=15, radius=2.1)\nkwargs = {'title': 'g(z) = sin(tan(z^2))', 'cmap': 'plasma_r'}\n\nplot_fractal(output, **kwargs);\n\nNext, let’s create a function that applies both f and g to the inputs each iteration and adds the result together:\n\nh(z) = f(z) + g(z) = tan(z^2) + sin(tan(z^2))\n\ndef h(z):\n    return f(z) + g(z)\n\n\n\noutput = general_julia(small_mesh, f=h, num_iter=10, radius=2.1)\nkwargs = {'title': 'h(z) = tan(z^2) + sin(tan(z^2))', 'figsize': (7, 7), 'extent': [-1, 1, -1, 1], 'cmap': 'jet'}\n\nplot_fractal(output, **kwargs);\n\nYou can even create beautiful fractals through your own errors. Here is one that got created accidently by making a mistake in computing the derivative of a Newton fractal:\n\ndef accident(z):\n    return z - (2 * np.power(np.tan(z), 2) / (np.sin(z) * np.cos(z)))\n\n\n\noutput = general_julia(mesh, f=accident, num_iter=15, c=0, radius=np.pi)\nkwargs = {'title': 'Accidental \\\\ fractal', 'cmap': 'Blues'}\n\nplot_fractal(output, **kwargs);\n\nNeedless to say, there are a nearly endless supply of interesting fractal creations that can be made just by playing around with various combinations of NumPy universal functions and by tinkering with the parameters.\n\n","type":"content","url":"/tutorial-plotting-fractals#creating-your-own-fractals","position":19},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"In conclusion"},"type":"lvl2","url":"/tutorial-plotting-fractals#in-conclusion","position":20},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"In conclusion"},"content":"We learned a lot about generating fractals today. We saw how complicated fractals requiring many iterations could be computed efficiently using universal functions. We also took advantage of boolean indexing, which allowed for less computations to be made without having to individually verify each value. Finally, we learned a lot about fractals themselves. As a recap:\n\nFractal images are created by iterating a function over a set of values, and keeping tally of how long it takes for each value to pass a certain threshold\n\nThe colours in the image correspond to the tally counts of the values\n\nThe filled-in Julia set for c consists of all complex numbers z in which f(z) = z^2 + c converges\n\nThe Julia set for c is the set of complex numbers that make up the boundary of the filled-in Julia set\n\nThe Mandelbrot set is all values c in which f(z) = z^2 + c converges at 0\n\nNewton fractals use functions of the form f(z) = z - \\frac{p(z)}{p'(z)}\n\nThe fractal images can vary as you adjust the number of iterations, radius of convergence, mesh size, colours, function choice and parameter choice\n\n","type":"content","url":"/tutorial-plotting-fractals#in-conclusion","position":21},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"On your own"},"type":"lvl2","url":"/tutorial-plotting-fractals#on-your-own","position":22},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"On your own"},"content":"Play around with the parameters of the generalized Julia set function, try playing with the constant value, number of iterations, function choice, radius, and colour choice.\n\nVisit the “List of fractals by Hausdorff dimension” Wikipedia page (linked in the Further reading section) and try writing a function for a fractal not mentioned in this tutorial.\n\n","type":"content","url":"/tutorial-plotting-fractals#on-your-own","position":23},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Further reading"},"type":"lvl2","url":"/tutorial-plotting-fractals#further-reading","position":24},{"hierarchy":{"lvl1":"Plotting Fractals","lvl2":"Further reading"},"content":"More information on the theory behind fractals\n\nFurther reading on Julia sets\n\nMore details about the Mandelbrot set\n\nA more complete treatment of Newton Fractals\n\nA list of different fractals","type":"content","url":"/tutorial-plotting-fractals#further-reading","position":25},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy"},"type":"lvl1","url":"/tutorial-static-equilibrium","position":0},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy"},"content":"When analyzing physical structures, it is crucial to understand the mechanics keeping them stable. Applied forces on a floor, a beam, or any other structure, create reaction forces and moments. These reactions are the structure resisting movement without breaking. In cases where structures do not move despite having forces applied to them, \n\nNewton’s second law states that both the acceleration and sum of forces in all directions in the system must be zero. You can represent and solve this concept with NumPy arrays.","type":"content","url":"/tutorial-static-equilibrium","position":1},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"What you’ll do:"},"type":"lvl2","url":"/tutorial-static-equilibrium#what-youll-do","position":2},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"What you’ll do:"},"content":"In this tutorial, you will use NumPy to create vectors and moments using NumPy arrays\n\nSolve problems involving cables and floors holding up structures\n\nWrite NumPy matrices to isolate unkowns\n\nUse NumPy functions to perform linear algebra operations","type":"content","url":"/tutorial-static-equilibrium#what-youll-do","position":3},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"What you’ll learn:"},"type":"lvl2","url":"/tutorial-static-equilibrium#what-youll-learn","position":4},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"What you’ll learn:"},"content":"How to represent points, vectors, and moments with NumPy.\n\nHow to find the \n\nnormal of vectors\n\nUsing NumPy to compute matrix calculations","type":"content","url":"/tutorial-static-equilibrium#what-youll-learn","position":5},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"What you’ll need:"},"type":"lvl2","url":"/tutorial-static-equilibrium#what-youll-need","position":6},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"What you’ll need:"},"content":"NumPy\n\nMatplotlib\n\nimported with the following comands:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nIn this tutorial you will use the following NumPy tools:\n\nnp.linalg.norm : this function determines the measure of vector magnitude\n\nnp.cross : this function takes two matrices and produces the cross product\n\n","type":"content","url":"/tutorial-static-equilibrium#what-youll-need","position":7},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Solving equilibrium with Newton’s second law"},"type":"lvl2","url":"/tutorial-static-equilibrium#solving-equilibrium-with-newtons-second-law","position":8},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Solving equilibrium with Newton’s second law"},"content":"Your model consists of a beam under a sum of forces and moments. You can start analyzing this system with Newton’s second law:\\sum{\\text{force}} = \\text{mass} \\times \\text{acceleration}.\n\nIn order to simplify the examples looked at, assume they are static, with acceleration =0. Due to our system existing in three dimensions, consider forces being applied in each of these dimensions. This means that you can represent these forces as vectors. You come to the same conclusion for \n\nmoments, which result from forces being applied a certain distance away from an object’s center of mass.\n\nAssume that the force F is represented as a three-dimensional vectorF = (F_x, F_y, F_z)\n\nwhere each of the three components represent the magnitude of the force being applied in each corresponding direction. Assume also that each component in the vectorr = (r_x, r_y, r_z)\n\nis the distance between the point where each component of the force is applied and the centroid of the system. Then, the moment can be computed byr \\times F = (r_x, r_y, r_z) \\times (F_x, F_y, F_z).\n\nStart with some simple examples of force vectors\n\nforceA = np.array([1, 0, 0])\nforceB = np.array([0, 1, 0])\nprint(\"Force A =\", forceA)\nprint(\"Force B =\", forceB)\n\nThis defines forceA as being a vector with magnitude of 1 in the x direction and forceB as magnitude 1 in the y direction.\n\nIt may be helpful to visualize these forces in order to better understand how they interact with each other.\nMatplotlib is a library with visualization tools that can be utilized for this purpose.\nQuiver plots will be used to demonstrate \n\nthree dimensional vectors, but the library can also be used for \n\ntwo dimensional demonstrations.\n\nfig = plt.figure()\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nx, y, z = np.array([0, 0, 0])  # defining the point of application.  Make it the origin\n\nu, v, w = forceA  # breaking the force vector into individual components\nd3.quiver(x, y, z, u, v, w, color=\"r\", label=\"forceA\")\n\nu, v, w = forceB\nd3.quiver(x, y, z, u, v, w, color=\"b\", label=\"forceB\")\n\nplt.legend()\nplt.show()\n\nThere are two forces emanating from a single point. In order to simplify this problem, you can add them together to find the sum of forces. Note that both forceA and forceB are three-dimensional vectors, represented by NumPy as arrays with three components. Because NumPy is meant to simplify and optimize operations between vectors, you can easily compute the sum of these two vectors as follows:\n\nforceC = forceA + forceB\nprint(\"Force C =\", forceC)\n\nForce C now acts as a single force that represents both A and B.\nYou can plot it to see the result.\n\nfig = plt.figure()\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nx, y, z = np.array([0, 0, 0])\n\nu, v, w = forceA\nd3.quiver(x, y, z, u, v, w, color=\"r\", label=\"forceA\")\nu, v, w = forceB\nd3.quiver(x, y, z, u, v, w, color=\"b\", label=\"forceB\")\nu, v, w = forceC\nd3.quiver(x, y, z, u, v, w, color=\"g\", label=\"forceC\")\n\nplt.legend()\nplt.show()\n\nHowever, the goal is equilibrium.\nThis means that you want your sum of forces to be (0, 0, 0) or else your object will experience acceleration.\nTherefore, there needs to be another force that counteracts the prior ones.\n\nYou can write this problem as A+B+R=0, with R being the reaction force that solves the problem.\n\nIn this example this would mean:(1, 0, 0) + (0, 1, 0) + (R_x, R_y, R_z) = (0, 0, 0)\n\nBroken into x, y, and z components this gives you:\\begin{cases}\n1+0+R_x=0\\\\\n0+1+R_y=0\\\\\n0+0+R_z=0\n\\end{cases}\n\nsolving for R_x, R_y, and R_z gives you a vector R of (-1, -1, 0).\n\nIf plotted, the forces seen in prior examples should be nullified.\nOnly if there is no force remaining is the system considered to be in equilibrium.\n\nR = np.array([-1, -1, 0])\n\nfig = plt.figure()\n\nd3.set_xlim(-1, 1)\nd3.set_ylim(-1, 1)\nd3.set_zlim(-1, 1)\n\nd3 = fig.add_subplot(projection=\"3d\")\n\nx, y, z = np.array([0, 0, 0])\n\nu, v, w = forceA + forceB + R  # add them all together for sum of forces\nd3.quiver(x, y, z, u, v, w)\n\nplt.show()\n\nThe empty graph signifies that there are no outlying forces. This denotes a system in equilibrium.","type":"content","url":"/tutorial-static-equilibrium#solving-equilibrium-with-newtons-second-law","position":9},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Solving Equilibrium as a sum of moments"},"type":"lvl2","url":"/tutorial-static-equilibrium#solving-equilibrium-as-a-sum-of-moments","position":10},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Solving Equilibrium as a sum of moments"},"content":"Next let’s move to a more complicated application.\nWhen forces are not all applied at the same point, moments are created.\n\nSimilar to forces, these moments must all sum to zero, otherwise rotational acceleration will be experienced.  Similar to the sum of forces, this creates a linear equation for each of the three coordinate directions in space.\n\nA simple example of this would be from a force applied to a stationary pole secured in the ground.\nThe pole does not move, so it must apply a reaction force.\nThe pole also does not rotate, so it must also be creating a reaction moment.\nSolve for both the reaction force and moments.\n\nLet’s say a 5N force is applied perpendicularly 2m above the base of the pole.\n\nf = 5  # Force in newtons\nL = 2  # Length of the pole\n\nR = 0 - f\nM = 0 - f * L\nprint(\"Reaction force =\", R)\nprint(\"Reaction moment =\", M)\n\n","type":"content","url":"/tutorial-static-equilibrium#solving-equilibrium-as-a-sum-of-moments","position":11},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Finding values with physical properties"},"type":"lvl2","url":"/tutorial-static-equilibrium#finding-values-with-physical-properties","position":12},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Finding values with physical properties"},"content":"Let’s say that instead of a force acting perpendicularly to the beam, a force was applied to our pole through a wire that was also attached to the ground.\nGiven the tension in this cord, all you need to solve this problem are the physical locations of these objects.\n\nIn response to the forces acting upon the pole, the base generated reaction forces in the x and y directions, as well as a reaction moment.\n\nDenote the base of the pole as the origin.\nNow, say the cord is attached to the ground 3m in the x direction and attached to the pole 2m up, in the z direction.\n\nDefine these points in space as NumPy arrays, and then use those arrays to find directional vectors.\n\npoleBase = np.array([0, 0, 0])\ncordBase = np.array([3, 0, 0])\ncordConnection = np.array([0, 0, 2])\n\npoleDirection = cordConnection - poleBase\nprint(\"Pole direction =\", poleDirection)\ncordDirection = cordBase - cordConnection\nprint(\"Cord direction =\", cordDirection)\n\nIn order to use these vectors in relation to forces you need to convert them into unit vectors.\nUnit vectors have a magnitude of one, and convey only the direction of the forces.\n\ncordUnit = cordDirection / np.linalg.norm(cordDirection)\nprint(\"Cord unit vector =\", cordUnit)\n\nYou can then multiply this direction with the magnitude of the force in order to find the force vector.\n\nLet’s say the cord has a tension of 5N:\n\ncordTension = 5\nforceCord = cordUnit * cordTension\nprint(\"Force from the cord =\", forceCord)\n\nIn order to find the moment you need the cross product of the force vector and the radius.\n\nmomentCord = np.cross(forceCord, poleDirection)\nprint(\"Moment from the cord =\", momentCord)\n\nNow all you need to do is find the reaction force and moment.\n\nequilibrium = np.array([0, 0, 0])\nR = equilibrium - forceCord\nM = equilibrium - momentCord\nprint(\"Reaction force =\", R)\nprint(\"Reaction moment =\", M)\n\n","type":"content","url":"/tutorial-static-equilibrium#finding-values-with-physical-properties","position":13},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl3":"Another Example","lvl2":"Finding values with physical properties"},"type":"lvl3","url":"/tutorial-static-equilibrium#another-example","position":14},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl3":"Another Example","lvl2":"Finding values with physical properties"},"content":"Let’s look at a slightly more complicated model.  In this example you will be observing a beam with two cables and an applied force.  This time you need to find both the tension in the cords and the reaction forces of the beam. (Source: \n\nVector Mechanics for Engineers: Statics and Dynamics, Problem 4.106)\n\nDefine distance a as 3 meters\n\nAs before, start by defining the location of each relevant point as an array.\n\nA = np.array([0, 0, 0])\nB = np.array([0, 3, 0])\nC = np.array([0, 6, 0])\nD = np.array([1.5, 0, -3])\nE = np.array([1.5, 0, 3])\nF = np.array([-3, 0, 2])\n\nFrom these equations, you start by determining vector directions with unit vectors.\n\nAB = B - C\nAC = C - A\nBD = D - B\nBE = E - B\nCF = F - C\n\nUnitBD = BD / np.linalg.norm(BD)\nUnitBE = BE / np.linalg.norm(BE)\nUnitCF = CF / np.linalg.norm(CF)\n\nRadBD = np.cross(AB, UnitBD)\nRadBE = np.cross(AB, UnitBE)\nRadCF = np.cross(AC, UnitCF)\n\nThis lets you represent the tension (T) and reaction (R) forces acting on the system as\\left[\n\\begin{array}\n~1/3 & 1/3 & 1 & 0 & 0\\\\\n-2/3 & -2/3 & 0 & 1 & 0\\\\\n-2/3 & 2/3 & 0 & 0 & 1\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n~T_{BD}\\\\\nT_{BE}\\\\\nR_{x}\\\\\nR_{y}\\\\\nR_{z}\\\\\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n~195\\\\\n390\\\\\n-130\\\\\n\\end{array}\n\\right]\n\nand the moments as\\left[\n\\begin{array}\n~2 & -2\\\\\n1 & 1\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n~T_{BD}\\\\\nT_{BE}\\\\\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n~780\\\\\n1170\\\\\n\\end{array}\n\\right]\n\nWhere T is the tension in the respective cord and R is the reaction force in a respective direction. Then you just have six equations:\n\n\\sum F_{x} = 0 = T_{BE}/3+T_{BD}/3-195+R_{x}\n\n\\sum F_{y} = 0 = (-\\frac{2}{3})T_{BE}-\\frac{2}{3}T_{BD}-390+R_{y}\n\n\\sum F_{z} = 0 = (-\\frac{2}{3})T_{BE}+\\frac{2}{3}T_{BD}+130+R_{z}\n\n\\sum M_{x} = 0 = 780+2T_{BE}-2T_{BD}\n\n\\sum M_{z} = 0 = 1170-T_{BE}-T_{BD}\n\nYou now have five unknowns with five equations, and can solve for:\n\n\\ T_{BD} = 780N\n\n\\ T_{BE} = 390N\n\n\\ R_{x} = -195N\n\n\\ R_{y} = 1170N\n\n\\ R_{z} = 130N\n\n","type":"content","url":"/tutorial-static-equilibrium#another-example","position":15},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Wrapping up"},"type":"lvl2","url":"/tutorial-static-equilibrium#wrapping-up","position":16},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl2":"Wrapping up"},"content":"You have learned how to use arrays to represent points, forces, and moments in three dimensional space. Each entry in an array can be used to represent a physical property broken into directional components. These can then be easily manipulated with NumPy functions.","type":"content","url":"/tutorial-static-equilibrium#wrapping-up","position":17},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl3":"Additional Applications","lvl2":"Wrapping up"},"type":"lvl3","url":"/tutorial-static-equilibrium#additional-applications","position":18},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl3":"Additional Applications","lvl2":"Wrapping up"},"content":"This same process can be applied to kinetic problems or in any number of dimensions. The examples done in this tutorial assumed three dimensional problems in static equilibrium. These methods can easily be used in more varied problems. More or less dimensions require larger or smaller arrays to represent. In systems experiencing acceleration, velocity and acceleration can be similarly be represented as vectors as well.","type":"content","url":"/tutorial-static-equilibrium#additional-applications","position":19},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl3":"References","lvl2":"Wrapping up"},"type":"lvl3","url":"/tutorial-static-equilibrium#references","position":20},{"hierarchy":{"lvl1":"Determining Static Equilibrium in NumPy","lvl3":"References","lvl2":"Wrapping up"},"content":"Vector Mechanics for Engineers: Statics and Dynamics (Beer & Johnston & Mazurek & et al.)\n\nNumPy Reference","type":"content","url":"/tutorial-static-equilibrium#references","position":21},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial"},"type":"lvl1","url":"/tutorial-style-guide","position":0},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial"},"content":"\n\nImage credit: Daniele Procida's \n\nDiátaxis framework, licensed under \n\nCC-BY-SA 4.0.\n\n","type":"content","url":"/tutorial-style-guide","position":1},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What you’ll do"},"type":"lvl2","url":"/tutorial-style-guide#what-youll-do","position":2},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What you’ll do"},"content":"Guided by a template, you’ll write a NumPy tutorial.","type":"content","url":"/tutorial-style-guide#what-youll-do","position":3},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What you’ll learn"},"type":"lvl2","url":"/tutorial-style-guide#what-youll-learn","position":4},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What you’ll learn"},"content":"You’ll be able to craft a tutorial that follows a standard format and reflects good teaching practice.\n\nYou’ll learn the three standard headings that open a NumPy tutorial -- What you’ll do, What you’ll learn, and What you’ll need -- and some optional headings for the bottom -- On your own, In practice, Further reading.\n\nYou’ll know what makes What you’ll learn different from What you’ll do.\n\nYou’ll be able to distinguish a tutorial from a how-to.\n\nYou’ll learn what not to put in a What you’ll learn section.","type":"content","url":"/tutorial-style-guide#what-youll-learn","position":5},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What you’ll need"},"type":"lvl2","url":"/tutorial-style-guide#what-youll-need","position":6},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What you’ll need"},"content":"This template.\n\nA portrait of your intended reader.\n\nJust as schools list prerequisites for higher-level courses, you can assume readers know some things (which you must list, as noted in the next bullet). Overexplaining bogs down the tutorial and obscures the main points.\n\nBut also put yourself in the reader’s place and consider what to explain along the way.\n\n“What you’ll need” is a list of:\n\npackages that must be present on the user’s machine before they begin. Don’t include numpy.\n\nwhat you assumed the reader knew in the bullet above. Don’t say Python;  familiarity with Python iterators is fine.\n\nInformality and enthusiasm. Imagine your reader not out in the audience but next to you.\n\nWillingness to write incomplete sentences for the What you’ll need bullets. They don’t begin with the words “You’ll need.”\n\nNot required are native English skills. Others can help.","type":"content","url":"/tutorial-style-guide#what-youll-need","position":7},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"After a horizontal rule, start your own headings"},"type":"lvl2","url":"/tutorial-style-guide#after-a-horizontal-rule-start-your-own-headings","position":8},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"After a horizontal rule, start your own headings"},"content":"Your tutorial steps begin here, using headings of your choice. At the end of the tutorial you’ll place another horizontal rule and return to standard headings.","type":"content","url":"/tutorial-style-guide#after-a-horizontal-rule-start-your-own-headings","position":9},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Titles have verbs"},"type":"lvl2","url":"/tutorial-style-guide#titles-have-verbs","position":10},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Titles have verbs"},"content":"In general, include a verb in the title; thus Learn to write a NumPy tutorial rather than “Rules for NumPy tutorials.” Consider putting verbs in the headings as well.","type":"content","url":"/tutorial-style-guide#titles-have-verbs","position":11},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Titles are lowercase"},"type":"lvl2","url":"/tutorial-style-guide#titles-are-lowercase","position":12},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Titles are lowercase"},"content":"Capitalize the first word, and after that only words that are ordinarily capitalized (so not “Titles Are Lowercase”).","type":"content","url":"/tutorial-style-guide#titles-are-lowercase","position":13},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What to say in “What you’ll learn”"},"type":"lvl2","url":"/tutorial-style-guide#what-to-say-in-what-youll-learn","position":14},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"What to say in “What you’ll learn”"},"content":"Avoid abstraction. “About” is a tipoff: Rather than writing “You’ll learn about NumPy I/O,” write “You’ll learn how to read a comma-delimited text file into a NumPy array.”","type":"content","url":"/tutorial-style-guide#what-to-say-in-what-youll-learn","position":15},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Why are “What you’ll do” and “What you’ll learn” different?"},"type":"lvl2","url":"/tutorial-style-guide#why-are-what-youll-do-and-what-youll-learn-different","position":16},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Why are “What you’ll do” and “What you’ll learn” different?"},"content":"What you’ll do is typically one sentence listing an end product: “You’ll bake a cake.” This makes the endpoint clear. What you’ll learn lists the payoffs, and there may be many: “You’ll learn to follow a recipe. You’ll get practice measuring ingredients. You’ll learn how to tell when a cake is ready to come out of the oven.”","type":"content","url":"/tutorial-style-guide#why-are-what-youll-do-and-what-youll-learn-different","position":17},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Avoid asides"},"type":"lvl2","url":"/tutorial-style-guide#avoid-asides","position":18},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Avoid asides"},"content":"As explained by expert documentation writer \n\nDaniele Procida:\n\nDon’t explain anything the learner doesn’t need to know in order to complete the tutorial.\n\nBecause tutorial steps are chosen to be clear and easy, they may fall short of\nproduction-grade. Yes, you should share this, but not during the tutorial, which should be straightforward and assured. The In practice section is the place for details, exceptions, alternatives, and similar fine print.","type":"content","url":"/tutorial-style-guide#avoid-asides","position":19},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Use plots and illustrations"},"type":"lvl2","url":"/tutorial-style-guide#use-plots-and-illustrations","position":20},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Use plots and illustrations"},"content":"Figures are a double win; they amplify your points and make the page inviting.  Like English skills, artistic skills (or graphic-toolset skills) aren’t required. Even if you only scan a hand illustration, somebody can polish it.\n\nAn illustration below the title, even if it’s only decorative, makes your page distinctive.","type":"content","url":"/tutorial-style-guide#use-plots-and-illustrations","position":21},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Use real datasets when possible"},"type":"lvl2","url":"/tutorial-style-guide#use-real-datasets-when-possible","position":22},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Use real datasets when possible"},"content":"Readers are likelier to be engaged by a real use case. Be sure you have rights to the data.","type":"content","url":"/tutorial-style-guide#use-real-datasets-when-possible","position":23},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Tutorials and how-to’s  -- similar but different"},"type":"lvl2","url":"/tutorial-style-guide#tutorials-and-how-tos-similar-but-different","position":24},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Tutorials and how-to’s  -- similar but different"},"content":"Tutorial readers are out-of-towners who want a feel for the place. Pick any single destination and explain sights along the way.\n\nUnlike how-to readers, who know what they need, tutorial readers don’t know what it is they don’t know. So while tutorials need headings like What you’ll do and What you’ll learn, these headings would never appear in a how-to.","type":"content","url":"/tutorial-style-guide#tutorials-and-how-tos-similar-but-different","position":25},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Make use of the Google doc style guide"},"type":"lvl2","url":"/tutorial-style-guide#make-use-of-the-google-doc-style-guide","position":26},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Make use of the Google doc style guide"},"content":"NumPy docs follow the \n\nGoogle developer documentation style guide. In addition to providing answers to recurring questions (“crossreference” or “cross-reference”?) the guide is filled with suggestions that will strengthen your doc writing.","type":"content","url":"/tutorial-style-guide#make-use-of-the-google-doc-style-guide","position":27},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"The notebook must be fully executable"},"type":"lvl2","url":"/tutorial-style-guide#the-notebook-must-be-fully-executable","position":28},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"The notebook must be fully executable"},"content":"Run all cells should execute all cells to the bottom of the file. If you’re demonstrating a bad expression and want to show the traceback, comment\nthe expression and put the traceback in a text cell.\n\n(Note that triple backquotes won’t be enough for a traceback that contains <text inside angle brackets>,\nthe angle brackets must be replaced by &lt; and &gt; as shown in the text cell markdown below.)\n\n# 100/0\n\n--------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-10-bbe761e74a70> in <module> ----> 1 100/0\n\nZeroDivisionError: division by zero\n\n","type":"content","url":"/tutorial-style-guide#the-notebook-must-be-fully-executable","position":29},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"On your own"},"type":"lvl2","url":"/tutorial-style-guide#on-your-own","position":30},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"On your own"},"content":"Close the tutorial section with a horizontal rule. You’re free to take any direction now, but here are three suggested sections.\n\nIn an optional On your own section, you can offer an assignment for readers to exercise their new skills. If it’s a question with an answer, provide it -- perhaps in a footnote to keep it from being a spoiler.","type":"content","url":"/tutorial-style-guide#on-your-own","position":31},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"In practice..."},"type":"lvl2","url":"/tutorial-style-guide#in-practice","position":32},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"In practice..."},"content":"The fine print that you avoided can go in this section.\n\nDon’t just say it’s usually done another way; explain why.","type":"content","url":"/tutorial-style-guide#in-practice","position":33},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Further reading"},"type":"lvl2","url":"/tutorial-style-guide#further-reading","position":34},{"hierarchy":{"lvl1":"Learn to write a NumPy tutorial","lvl2":"Further reading"},"content":"Ideally, rather than giving bare links, Further reading describes the references: \n\nThe Documentation System is the inspiration for this tutorial, and describes three other kinds of documentation.\n\nThe Google guide is long; there’s also \n\na summary.\n\nNumPy’s website includes a \n\ndocumentation how-to.","type":"content","url":"/tutorial-style-guide#further-reading","position":35},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays"},"type":"lvl1","url":"/tutorial-svd","position":0},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays"},"content":"","type":"content","url":"/tutorial-svd","position":1},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Prerequisites"},"type":"lvl2","url":"/tutorial-svd#prerequisites","position":2},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Prerequisites"},"content":"Before reading this tutorial, you should know a bit of Python. If you would like to refresh your memory, take a look at the \n\nPython tutorial.\n\nIf you want to be able to run the examples in this tutorial, you should also have \n\nmatplotlib and \n\nSciPy installed on your computer.","type":"content","url":"/tutorial-svd#prerequisites","position":3},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Learner profile"},"type":"lvl2","url":"/tutorial-svd#learner-profile","position":4},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Learner profile"},"content":"This tutorial is for people who have a basic understanding of linear algebra and arrays in NumPy and want to understand how n-dimensional (n>=2) arrays are represented and can be manipulated. In particular, if you don’t know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this tutorial might be of help.","type":"content","url":"/tutorial-svd#learner-profile","position":5},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Learning Objectives"},"type":"lvl2","url":"/tutorial-svd#learning-objectives","position":6},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Learning Objectives"},"content":"After this tutorial, you should be able to:\n\nUnderstand the difference between one-, two- and n-dimensional arrays in NumPy;\n\nUnderstand how to apply some linear algebra operations to n-dimensional arrays without using for-loops;\n\nUnderstand axis and shape properties for n-dimensional arrays.","type":"content","url":"/tutorial-svd#learning-objectives","position":7},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Content"},"type":"lvl2","url":"/tutorial-svd#content","position":8},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Content"},"content":"In this tutorial, we will use a \n\nmatrix decomposition from linear algebra, the Singular Value Decomposition, to generate a compressed approximation of an image. We’ll use the face image from the \n\nscipy.datasets module:\n\nfrom scipy.datasets import face\n\nimg = face()\n\nNote\n\nIf you prefer, you can use your own image as you work through this tutorial.\nIn order to transform your image into a NumPy array that can be manipulated, you\ncan use the imread function from the\n\n\nmatplotlib.pyplot submodule.\nAlternatively, you can use the\n\n\nimageio.imread\nfunction from the imageio library.\nBe aware that if you use your own image, you’ll likely need to adapt the steps below.\nFor more information on how images are treated when converted to NumPy arrays,\nsee \n\nA crash course on NumPy for images from the scikit-image documentation.\n\nNow, img is a NumPy array, as we can see when using the type function:\n\ntype(img)\n\nWe can see the image using the \n\nmatplotlib​.pyplot​.imshow function & the special iPython command, %matplotlib inline to display plots inline:\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n\nplt.imshow(img)\nplt.show()\n\n","type":"content","url":"/tutorial-svd#content","position":9},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Shape, axis and array properties","lvl2":"Content"},"type":"lvl3","url":"/tutorial-svd#shape-axis-and-array-properties","position":10},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Shape, axis and array properties","lvl2":"Content"},"content":"Note that, in linear algebra, the dimension of a vector refers to the number of entries in an array. In NumPy, it instead defines the number of axes. For example, a 1D array is a vector such as [1, 2, 3], a 2D array is a matrix, and so forth.\n\nFirst, let’s check for the shape of the data in our array. Since this image is two-dimensional (the pixels in the image form a rectangle), we might expect a two-dimensional array to represent it (a matrix). However, using the shape property of this NumPy array gives us a different result:\n\nimg.shape\n\nThe output is a \n\ntuple with three elements, which means that this is a three-dimensional array. Since this is a color image, and we have used the imread function to read it, the data is organized as a 768×1024 grid of pixels, where each pixel contains 3 values representing color channels (red, green and blue - RGB). You can see this by looking at the shape, where the leftmost number corresponds to the outermost axis (image height), the middle number to the next axis (image width) and the rightmost number to the innermost axis (the color channels).\n\nFurthermore, using the ndim property of this array, we can see that\n\nimg.ndim\n\nNumPy refers to each dimension as an axis. Because of how imread works, the first index in the 3rd axis is the red pixel data for our image. We can access this by using the syntax\n\nimg[:, :, 0]\n\nFrom the output above, we can see that every value in img[:, :, 0] is an integer value between 0 and 255, representing the level of red in each corresponding image pixel (keep in mind that this might be different if you\nuse your own image instead of \n\nscipy.datasets.face).\n\nAs expected, this is a 768x1024 matrix:\n\nimg[:, :, 0].shape\n\nSince we are going to perform linear algebra operations on this data, it might be more interesting to have real numbers between 0 and 1 in each entry of the matrices to represent the RGB values. We can do that by setting\n\nimg_array = img / 255\n\nThis operation, dividing an array by a scalar, works because of NumPy’s \n\nbroadcasting rules.\n\nTip\n\nIn real-world applications, it may be better to use, for example, the\n\n\nimg_as_float\nutility function from scikit-image.\n\nYou can check that the above works by doing some tests; for example, inquiring\nabout maximum and minimum values for this array:\n\nimg_array.min(), img_array.max()\n\nor checking the type of data in the array:\n\nimg_array.dtype\n\nNote that we can assign each color channel to a separate matrix using the slice syntax:\n\nred_array = img_array[:, :, 0]\ngreen_array = img_array[:, :, 1]\nblue_array = img_array[:, :, 2]\n\n","type":"content","url":"/tutorial-svd#shape-axis-and-array-properties","position":11},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Operations on an axis","lvl2":"Content"},"type":"lvl3","url":"/tutorial-svd#operations-on-an-axis","position":12},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Operations on an axis","lvl2":"Content"},"content":"It is possible to use methods from linear algebra to approximate an existing set of data. Here, we will use the \n\nSVD (Singular Value Decomposition) to try to rebuild an image that uses less singular value information than the original one, while still retaining some of its features.\n\nNote\n\nWe will use NumPy’s linear algebra module,\n\n\nnumpy.linalg,\nto perform the operations in this tutorial.\nMost of the linear algebra functions in this module can also be found in\n\n\nscipy.linalg,\nand users are encouraged to use the \n\nscipy\nmodule for real-world applications.\nHowever, some functions in the\n\n\nscipy.linalg\nmodule, such as the SVD function, only support 2D arrays.\nFor more information on this, check the \n\nscipy.linalg page.\n\nIn order to extract information from a given matrix, we can use the SVD to obtain 3 arrays which can be multiplied to obtain the original matrix. From the theory of linear algebra, given a matrix A, the following product can be computed:U \\Sigma V^T = A\n\nwhere U and V^T are square and \\Sigma is the same size as A. \\Sigma is a diagonal matrix and contains the \n\nsingular values of A, organized from largest to smallest. These values are always non-negative and can be used as an indicator of the “importance” of some features represented by the matrix A.\n\nLet’s see how this works in practice with just one matrix first. Note that according to \n\ncolorimetry,\nit is possible to obtain a fairly reasonable grayscale version of our color image if we apply the formulaY = 0.2126 R + 0.7152 G + 0.0722 B\n\nwhere Y is the array representing the grayscale image, and R, G and B are the red, green and blue channel arrays we had originally. Notice we can use the @ operator (the matrix multiplication operator for NumPy arrays, see \n\nnumpy.matmul) for this:\n\nimg_gray = img_array @ [0.2126, 0.7152, 0.0722]\n\nNow, img_gray has shape\n\nimg_gray.shape\n\nTo see if this makes sense in our image, we should use a colormap from matplotlib corresponding to the color we wish to see in out image (otherwise, matplotlib will default to a colormap that does not correspond to the real data).\n\nIn our case, we are approximating the grayscale portion of the image, so we will use the colormap gray:\n\nplt.imshow(img_gray, cmap=\"gray\")\nplt.show()\n\nNow, applying the \n\nlinalg.svd function to this matrix, we obtain the following decomposition:\n\nimport numpy as np\nU, s, Vt = np.linalg.svd(img_gray)\n\nNote\n\nIf you are using your own image, this command might take a while to run,\ndepending on the size of your image and your hardware.\nDon’t worry, this is normal! The SVD can be a pretty intensive computation.\n\nLet’s check that this is what we expected:\n\nU.shape, s.shape, Vt.shape\n\nNote that s has a particular shape: it has only one dimension. This means that some linear algebra functions that expect 2d arrays might not work. For example, from the theory, one might expect s and Vt to be\ncompatible for multiplication. However, this is not true as s does not have a second axis:\n\ns @ Vt\n\nresults in a ValueError. This happens because having a one-dimensional array for s, in this case, is much more economic in practice than building a diagonal matrix with the same data. To reconstruct the original matrix, we can rebuild the diagonal matrix \\Sigma with the elements of s in its diagonal and with the appropriate dimensions for multiplying: in our case, \\Sigma should be 768x1024 since U is 768x768 and Vt is 1024x1024. In order to add the singular values to the diagonal of Sigma, we will use the \n\nfill_diagonal function from NumPy:\n\nSigma = np.zeros((U.shape[1], Vt.shape[0]))\nnp.fill_diagonal(Sigma, s)\n\nNow, we want to check if the reconstructed U @ Sigma @ Vt is close to the original img_gray matrix.\n\n","type":"content","url":"/tutorial-svd#operations-on-an-axis","position":13},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Approximation"},"type":"lvl2","url":"/tutorial-svd#approximation","position":14},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Approximation"},"content":"The \n\nlinalg module includes a norm function, which computes the norm of a vector or matrix represented in a NumPy array. For example, from the SVD explanation above, we would expect the norm of the difference between img_gray and the reconstructed SVD product to be small. As expected, you should see something like\n\nnp.linalg.norm(img_gray - U @ Sigma @ Vt)\n\n(The actual result of this operation might be different depending on your architecture and linear algebra setup. Regardless, you should see a small number.)\n\nWe could also have used the \n\nnumpy.allclose function to make sure the reconstructed product is, in fact, close to our original matrix (the difference between the two arrays is small):\n\nnp.allclose(img_gray, U @ Sigma @ Vt)\n\nTo see if an approximation is reasonable, we can check the values in s:\n\nplt.plot(s)\nplt.show()\n\nIn the graph, we can see that although we have 768 singular values in s, most of those (after the 150th entry or so) are pretty small. So it might make sense to use only the information related to the first (say, 50) singular values to build a more economical approximation to our image.\n\nThe idea is to consider all but the first k singular values in Sigma (which are the same as in s) as zeros, keeping U and Vt intact, and computing the product of these matrices as the approximation.\n\nFor example, if we choose\n\nk = 10\n\nwe can build the approximation by doing\n\napprox = U @ Sigma[:, :k] @ Vt[:k, :]\n\nNote that we had to use only the first k rows of Vt, since all other rows would be multiplied by the zeros corresponding to the singular values we eliminated from this approximation.\n\nplt.imshow(approx, cmap=\"gray\")\nplt.show()\n\nNow, you can go ahead and repeat this experiment with other values of k, and each of your experiments should give you a slightly better (or worse) image depending on the value you choose.\n\n","type":"content","url":"/tutorial-svd#approximation","position":15},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Applying to all colors","lvl2":"Approximation"},"type":"lvl3","url":"/tutorial-svd#applying-to-all-colors","position":16},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Applying to all colors","lvl2":"Approximation"},"content":"Now we want to do the same kind of operation, but to all three colors. Our first instinct might be to repeat the same operation we did above to each color matrix individually. However, NumPy’s broadcasting takes care of this\nfor us.\n\nIf our array has more than two dimensions, then the SVD can be applied to all axes at once. However, the linear algebra functions in NumPy expect to see an array of the form (n, M, N), where the first axis n represents the number of MxN matrices in the stack.\n\nIn our case,\n\nimg_array.shape\n\nso we need to permutate the axis on this array to get a shape like (3, 768, 1024). Fortunately, the \n\nnumpy.transpose function can do that for us:\n\n# The values in the tuple indicate the original dim, and the order the new axis\n# so axis 2 -> 0, 0 -> 1, and 1 -> 2\nimg_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape\n\nNow we are ready to apply the SVD:\n\nU, s, Vt = np.linalg.svd(img_array_transposed)\n\nFinally, to obtain the full approximated image, we need to reassemble these matrices into the approximation. Now, note that\n\nU.shape, s.shape, Vt.shape\n\nTo build the final approximation matrix, we must understand how multiplication across different axes works.\n\n","type":"content","url":"/tutorial-svd#applying-to-all-colors","position":17},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Products with n-dimensional arrays","lvl2":"Approximation"},"type":"lvl3","url":"/tutorial-svd#products-with-n-dimensional-arrays","position":18},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Products with n-dimensional arrays","lvl2":"Approximation"},"content":"If you have worked before with only one- or two-dimensional arrays in NumPy, you might use \n\nnumpy.dot and \n\nnumpy.matmul (or the @ operator) interchangeably. However, for n-dimensional arrays, they work in very different ways. For more details, check the documentation on \n\nnumpy.matmul.\n\nNow, to build our approximation, we first need to make sure that our singular values are ready for multiplication, so we build our Sigma matrix similarly to what we did before. The Sigma array must have dimensions (3, 768, 1024). In order to add the singular values to the diagonal of Sigma, we will again use the \n\nfill_diagonal function, using each of the 3 rows in s as the diagonal for each of the 3 matrices in Sigma:\n\nSigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])\n\nNow, if we wish to rebuild the full SVD (with no approximation), we can do\n\nreconstructed = U @ Sigma @ Vt\n\nNote that\n\nreconstructed.shape\n\nThe reconstructed image should be indistinguishable from the original one, except for differences due to floating point errors from the reconstruction. Recall that our original image consisted of floating point values in the range [0., 1.]. The accumulation of floating point error from the reconstruction can result in values slightly outside this original range:\n\nreconstructed.min(), reconstructed.max()\n\nSince imshow expects values in the range, we can use clip to excise the floating point error:\n\nreconstructed = np.clip(reconstructed, 0, 1)\nplt.imshow(np.transpose(reconstructed, (1, 2, 0)))\nplt.show()\n\nNote\n\nIn fact, imshow peforms this clipping under-the-hood, so if you skip the first\nline in the previous code cell, you might see a warning message saying\n\"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\"\n\nNow, to do the approximation, we must choose only the first k singular values for each color channel. This can be done using the following syntax:\n\napprox_img = U @ Sigma[..., :k] @ Vt[..., :k, :]\n\nYou can see that we have selected only the first k components of the last axis for Sigma (this means that we have used only the first k columns of each of the three matrices in the stack), and that we have selected only the first k components in the second-to-last axis of Vt (this means we have selected only the first k rows from every matrix in the stack Vt and all columns). If you are unfamiliar with the ellipsis syntax, it is a\nplaceholder for other axes. For more details, see the documentation on \n\nIndexing.\n\nNow,\n\napprox_img.shape\n\nwhich is not the right shape for showing the image. Finally, reordering the axes back to our original shape of (768, 1024, 3), we can see our approximation:\n\nplt.imshow(np.transpose(np.clip(approx_img, 0, 1), (1, 2, 0)))\nplt.show()\n\nEven though the image is not as sharp, using a small number of k singular values (compared to the original set of 768 values), we can recover many of the distinguishing features from this image.\n\n","type":"content","url":"/tutorial-svd#products-with-n-dimensional-arrays","position":19},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Final words","lvl2":"Approximation"},"type":"lvl3","url":"/tutorial-svd#final-words","position":20},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl3":"Final words","lvl2":"Approximation"},"content":"Of course, this is not the best method to approximate an image. However, there is, in fact, a result in linear algebra that says that the approximation we built above is the best we can get to the original matrix in\nterms of the norm of the difference. For more information, see G. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985.","type":"content","url":"/tutorial-svd#final-words","position":21},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Further reading"},"type":"lvl2","url":"/tutorial-svd#further-reading","position":22},{"hierarchy":{"lvl1":"Linear algebra on n-dimensional arrays","lvl2":"Further reading"},"content":"Python tutorial\n\nNumPy Reference\n\nSciPy Tutorial\n\nSciPy Lecture Notes\n\nA matlab, R, IDL, NumPy/SciPy dictionary","type":"content","url":"/tutorial-svd#further-reading","position":23},{"hierarchy":{"lvl1":"X-ray image processing"},"type":"lvl1","url":"/tutorial-x-ray-image-processing","position":0},{"hierarchy":{"lvl1":"X-ray image processing"},"content":"This tutorial demonstrates how to read and process X-ray images with NumPy,\nimageio, Matplotlib and SciPy. You will learn how to load medical images, focus\non certain parts, and visually compare them using the\n\n\nGaussian,\n\n\nLaplacian-Gaussian,\n\n\nSobel, and\n\n\nCanny filters for edge\ndetection.\n\nX-ray image analysis can be part of your data analysis and\n\n\nmachine learning workflow\nwhen, for example, you’re building an algorithm that helps\n\n\ndetect pneumonia\nas part of a \n\nKaggle\n\n\ncompetition.\nIn the healthcare industry, medical image processing and analysis is\nparticularly important when images are estimated to account for\n\n\nat least 90% of all\nmedical data.\n\nYou’ll be working with radiology images from the\n\n\nChestX-ray8\ndataset provided by the \n\nNational Institutes of Health (NIH).\nChestX-ray8 contains over 100,000 de-identified X-ray images in the PNG format\nfrom more than 30,000 patients. You can find ChestX-ray8’s files on NIH’s public\nBox \n\nrepository in the /images\nfolder. (For more details, refer to the research\n\n\npaper\npublished at CVPR (a computer vision conference) in 2017.)\n\nFor your convenience, a small number of PNG images have been saved to this\ntutorial’s repository under tutorial-x-ray-image-processing/, since\nChestX-ray8 contains gigabytes of data and you may find it challenging to\ndownload it in batches.\n\n","type":"content","url":"/tutorial-x-ray-image-processing","position":1},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Prerequisites"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#prerequisites","position":2},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Prerequisites"},"content":"\n\nThe reader should have some knowledge of Python, NumPy arrays, and Matplotlib.\nTo refresh the memory, you can take the\n\n\nPython and Matplotlib\n\n\nPyPlot tutorials,\nand the NumPy \n\nquickstart.\n\nThe following packages are used in this tutorial:\n\nimageio for reading and writing image data. The\nhealthcare industry usually works with the\n\n\nDICOM format for medical imaging and\n\n\nimageio should be\nwell-suited for reading that format. For simplicity, in this tutorial you’ll be\nworking with PNG files.\n\nMatplotlib for data visualization.\n\nSciPy for multi-dimensional image processing via\n\n\nndimage.\n\nThis tutorial can be run locally in an isolated environment, such as\n\n\nVirtualenv or\n\n\nconda.\nYou can use \n\nJupyter Notebook or JupyterLab to run\neach notebook cell.\n\n","type":"content","url":"/tutorial-x-ray-image-processing#prerequisites","position":3},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Table of contents"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#table-of-contents","position":4},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Table of contents"},"content":"\n\nExamine an X-ray with imageio\n\nCombine images into a multi-dimensional array to demonstrate progression\n\nEdge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and\nCanny filters\n\nApply masks to X-rays with np.where()\n\nCompare the results\n\n","type":"content","url":"/tutorial-x-ray-image-processing#table-of-contents","position":5},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Examine an X-ray with imageio"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#examine-an-x-ray-with-imageio","position":6},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Examine an X-ray with imageio"},"content":"\n\nLet’s begin with a simple example using just one X-ray image from the\nChestX-ray8 dataset.\n\nThe file — 00000011_001.png — has been downloaded for you and saved in the\n/tutorial-x-ray-image-processing folder.\n\n1. Load the image with imageio:\n\nimport os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))\n\n2. Check that its shape is 1024x1024 pixels and that the array is made up of\n8-bit integers:\n\nprint(xray_image.shape)\nprint(xray_image.dtype)\n\n3. Import matplotlib and display the image in a grayscale colormap:\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(xray_image, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#examine-an-x-ray-with-imageio","position":7},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Combine images into a multidimensional array to demonstrate progression"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#combine-images-into-a-multidimensional-array-to-demonstrate-progression","position":8},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Combine images into a multidimensional array to demonstrate progression"},"content":"\n\nIn the next example, instead of 1 image you’ll use 9 X-ray 1024x1024-pixel\nimages from the ChestX-ray8 dataset that have been downloaded and extracted\nfrom one of the dataset files. They are numbered from ...000.png to\n...008.png and let’s assume they belong to the same patient.\n\n1. Import NumPy, read in each of the X-rays, and create a three-dimensional\narray where the first dimension corresponds to image number:\n\nimport numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)\n\n2. Check the shape of the new X-ray image array containing 9 stacked images:\n\ncombined_xray_images_1.shape\n\nNote that the shape in the first dimension matches num_imgs, so the\ncombined_xray_images_1 array can be interpreted as a stack of 2D images.\n\n3. You can now display the “health progress” by plotting each of frames next\nto each other using Matplotlib:\n\nfig, axes = plt.subplots(nrows=1, ncols=num_imgs, figsize=(30, 30))\n\nfor img, ax in zip(combined_xray_images_1, axes):\n    ax.imshow(img, cmap='gray')\n    ax.axis('off')\n\n4. In addition, it can be helpful to show the progress as an animation.\nLet’s create a GIF file with imageio.mimwrite() and display the result in the\nnotebook:\n\nGIF_PATH = os.path.join(DIR, \"xray_image.gif\")\nimageio.mimwrite(GIF_PATH, combined_xray_images_1, format= \".gif\", duration=1000)\n\nWhich gives us:\n","type":"content","url":"/tutorial-x-ray-image-processing#combine-images-into-a-multidimensional-array-to-demonstrate-progression","position":9},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#edge-detection-using-the-laplacian-gaussian-gaussian-gradient-sobel-and-canny-filters","position":10},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"content":"\n\nWhen processing biomedical data, it can be useful to emphasize the 2D\n\n\n“edges” to focus on particular\nfeatures in an image. To do that, using\n\n\nimage gradients can be\nparticularly helpful when detecting the change of color pixel intensity.\n\n","type":"content","url":"/tutorial-x-ray-image-processing#edge-detection-using-the-laplacian-gaussian-gaussian-gradient-sobel-and-canny-filters","position":11},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Laplace filter with Gaussian second derivatives","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"type":"lvl3","url":"/tutorial-x-ray-image-processing#the-laplace-filter-with-gaussian-second-derivatives","position":12},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Laplace filter with Gaussian second derivatives","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"content":"Let’s start with an n-dimensional\n\n\nLaplace filter\n(“Laplacian-Gaussian”) that uses\n\n\nGaussian second\nderivatives. This Laplacian method focuses on pixels with rapid intensity change\nin values and is combined with Gaussian smoothing to\n\n\nremove noise. Let’s examine\nhow it can be useful in analyzing 2D X-ray images.\n\nThe implementation of the Laplacian-Gaussian filter is relatively\nstraightforward: 1) import the ndimage module from SciPy; and 2) call\n\n\nscipy.ndimage.gaussian_laplace()\nwith a sigma (scalar) parameter, which affects the standard deviations of the\nGaussian filter (you’ll use 1 in the example below):\n\nfrom scipy import ndimage\n\nxray_image_laplace_gaussian = ndimage.gaussian_laplace(xray_image, sigma=1)\n\nDisplay the original X-ray and the one with the Laplacian-Gaussian filter:\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplacian-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#the-laplace-filter-with-gaussian-second-derivatives","position":13},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Gaussian gradient magnitude method","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"type":"lvl3","url":"/tutorial-x-ray-image-processing#the-gaussian-gradient-magnitude-method","position":14},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Gaussian gradient magnitude method","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"content":"Another method for edge detection that can be useful is the\n\n\nGaussian (gradient) filter.\nIt computes the multidimensional gradient magnitude with Gaussian derivatives\nand helps by remove\n\n\nhigh-frequency\nimage components.\n\n1. Call \n\nscipy.ndimage.gaussian_gradient_magnitude()\nwith a sigma (scalar) parameter (for standard deviations; you’ll use 2 in the\nexample below):\n\nx_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)\n\n2. Display the original X-ray and the one with the Gaussian gradient filter:\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Gaussian gradient (edges)\")\naxes[1].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#the-gaussian-gradient-magnitude-method","position":15},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Sobel-Feldman operator (the Sobel filter)","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"type":"lvl3","url":"/tutorial-x-ray-image-processing#the-sobel-feldman-operator-the-sobel-filter","position":16},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Sobel-Feldman operator (the Sobel filter)","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"content":"To find regions of high spatial frequency (the edges or the edge maps) along the\nhorizontal and vertical axes of a 2D X-ray image, you can use the\n\n\nSobel-Feldman operator (Sobel filter)\ntechnique. The Sobel filter applies two 3x3 kernel matrices — one for each axis\n— onto the X-ray through a \n\nconvolution.\nThen, these two points (gradients) are combined using the\n\n\nPythagorean theorem to\nproduce a gradient magnitude.\n\n1. Use the Sobel filters — (\n\nscipy.ndimage.sobel())\n— on x- and y-axes of the X-ray. Then, calculate the distance between x and\ny (with the Sobel filters applied to them) using the\n\n\nPythagorean theorem and\nNumPy’s \n\nnp.hypot()\nto obtain the magnitude. Finally, normalize the rescaled image for the pixel\nvalues to be between 0 and 255.\n\nImage normalization\nfollows the output_channel = 255.0 * (input_channel - min_value) / (max_value - min_value)\n\n\nformula. Because you’re\nusing a grayscale image, you need to normalize just one channel.\n\nx_sobel = ndimage.sobel(xray_image, axis=0)\ny_sobel = ndimage.sobel(xray_image, axis=1)\n\nxray_image_sobel = np.hypot(x_sobel, y_sobel)\n\nxray_image_sobel *= 255.0 / np.max(xray_image_sobel)\n\n2. Change the new image array data type to the 32-bit floating-point format\nfrom float16 to \n\nmake it compatible\nwith Matplotlib:\n\nprint(\"The data type - before: \", xray_image_sobel.dtype)\n\nxray_image_sobel = xray_image_sobel.astype(\"float32\")\n\nprint(\"The data type - after: \", xray_image_sobel.dtype)\n\n3. Display the original X-ray and the one with the Sobel “edge” filter\napplied. Note that both the grayscale and CMRmap colormaps are used to help\nemphasize the edges:\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Sobel (edges) - grayscale\")\naxes[1].imshow(xray_image_sobel, cmap=\"gray\")\naxes[2].set_title(\"Sobel (edges) - CMRmap\")\naxes[2].imshow(xray_image_sobel, cmap=\"CMRmap\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#the-sobel-feldman-operator-the-sobel-filter","position":17},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Canny filter","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"type":"lvl3","url":"/tutorial-x-ray-image-processing#the-canny-filter","position":18},{"hierarchy":{"lvl1":"X-ray image processing","lvl3":"The Canny filter","lvl2":"Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters"},"content":"You can also consider using another well-known filter for edge detection called\nthe \n\nCanny filter.\n\nFirst, you apply a \n\nGaussian\nfilter to remove the noise in an image. In this example, you’re using using the\n\n\nFourier filter which\nsmoothens the X-ray through a \n\nconvolution\nprocess. Next, you apply the \n\nPrewitt filter\non each of the 2 axes of the image to help detect some of the edges — this will\nresult in 2 gradient values. Similar to the Sobel filter, the Prewitt operator\nalso applies two 3x3 kernel matrices — one for each axis — onto the X-ray\nthrough a \n\nconvolution.\nIn the end, you compute the magnitude between the two gradients using the\n\n\nPythagorean theorem and\n\n\nnormalize\nthe images, as before.\n\n1. Use SciPy’s Fourier filters — \n\nscipy.ndimage.fourier_gaussian()\n— with a small sigma value to remove some of the noise from the X-ray. Then,\ncalculate two gradients using \n\nscipy.ndimage.prewitt().\nNext, measure the distance between the gradients using NumPy’s np.hypot().\nFinally, \n\nnormalize\nthe rescaled image, as before.\n\nfourier_gaussian = ndimage.fourier_gaussian(xray_image, sigma=0.05)\n\nx_prewitt = ndimage.prewitt(fourier_gaussian, axis=0)\ny_prewitt = ndimage.prewitt(fourier_gaussian, axis=1)\n\nxray_image_canny = np.hypot(x_prewitt, y_prewitt)\n\nxray_image_canny *= 255.0 / np.max(xray_image_canny)\n\nprint(\"The data type - \", xray_image_canny.dtype)\n\n2. Plot the original X-ray image and the ones with the edges detected with\nthe help of the Canny filter technique. The edges can be emphasized using the\nprism, nipy_spectral, and terrain Matplotlib colormaps.\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 15))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Canny (edges) - prism\")\naxes[1].imshow(xray_image_canny, cmap=\"prism\")\naxes[2].set_title(\"Canny (edges) - nipy_spectral\")\naxes[2].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[3].set_title(\"Canny (edges) - terrain\")\naxes[3].imshow(xray_image_canny, cmap=\"terrain\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#the-canny-filter","position":19},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Apply masks to X-rays with np.where()"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#apply-masks-to-x-rays-with-np-where","position":20},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Apply masks to X-rays with np.where()"},"content":"\n\nTo screen out only certain pixels in X-ray images to help detect particular\nfeatures, you can apply masks with NumPy’s\n\n\nnp.where(condition: array_like (bool), x: array_like, y: ndarray)\nthat returns x when True and y when False.\n\nIdentifying regions of interest — certain sets of pixels in an image — can be\nuseful and masks serve as boolean arrays of the same shape as the original\nimage.\n\n1. Retrieve some basics statistics about the pixel values in the original\nX-ray image you’ve been working with:\n\nprint(\"The data type of the X-ray image is: \", xray_image.dtype)\nprint(\"The minimum pixel value is: \", np.min(xray_image))\nprint(\"The maximum pixel value is: \", np.max(xray_image))\nprint(\"The average pixel value is: \", np.mean(xray_image))\nprint(\"The median pixel value is: \", np.median(xray_image))\n\n2. The array data type is uint8 and the minimum/maximum value results\nsuggest that all 256 colors (from 0 to 255) are used in the X-ray. Let’s\nvisualize the pixel intensity distribution of the original raw X-ray image\nwith ndimage.histogram() and Matplotlib:\n\npixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\nAs the pixel intensity distribution suggests, there are many low (between around\n0 and 20) and very high (between around 200 and 240) pixel values.\n\n3. You can create different conditional masks with NumPy’s np.where() —\nfor example, let’s have only those values of the image with the pixels exceeding\na certain threshold:\n\n# The threshold is \"greater than 150\"\n# Return the original image if true, `0` otherwise\nxray_image_mask_noisy = np.where(xray_image > 150, xray_image, 0)\n\nplt.imshow(xray_image_mask_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n# The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image > 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#apply-masks-to-x-rays-with-np-where","position":21},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Compare the results"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#compare-the-results","position":22},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Compare the results"},"content":"\n\nLet’s display some of the results of processed X-ray images you’ve worked with\nso far:\n\nfig, axes = plt.subplots(nrows=1, ncols=9, figsize=(30, 30))\n\naxes[0].set_title(\"Original\")\naxes[0].imshow(xray_image, cmap=\"gray\")\naxes[1].set_title(\"Laplace-Gaussian (edges)\")\naxes[1].imshow(xray_image_laplace_gaussian, cmap=\"gray\")\naxes[2].set_title(\"Gaussian gradient (edges)\")\naxes[2].imshow(x_ray_image_gaussian_gradient, cmap=\"gray\")\naxes[3].set_title(\"Sobel (edges) - grayscale\")\naxes[3].imshow(xray_image_sobel, cmap=\"gray\")\naxes[4].set_title(\"Sobel (edges) - hot\")\naxes[4].imshow(xray_image_sobel, cmap=\"hot\")\naxes[5].set_title(\"Canny (edges) - prism)\")\naxes[5].imshow(xray_image_canny, cmap=\"prism\")\naxes[6].set_title(\"Canny (edges) - nipy_spectral)\")\naxes[6].imshow(xray_image_canny, cmap=\"nipy_spectral\")\naxes[7].set_title(\"Mask (> 150, noisy)\")\naxes[7].imshow(xray_image_mask_noisy, cmap=\"gray\")\naxes[8].set_title(\"Mask (> 150, less noisy)\")\naxes[8].imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nfor i in axes:\n    i.axis(\"off\")\nplt.show()\n\n","type":"content","url":"/tutorial-x-ray-image-processing#compare-the-results","position":23},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Next steps"},"type":"lvl2","url":"/tutorial-x-ray-image-processing#next-steps","position":24},{"hierarchy":{"lvl1":"X-ray image processing","lvl2":"Next steps"},"content":"\n\nIf you want to use your own samples, you can use\n\n\nthis image\nor search for various other ones on the \n\nOpeni\ndatabase. Openi contains many biomedical images and it can be especially helpful\nif you have low bandwidth and/or are restricted by the amount of data you can\ndownload.\n\nTo learn more about image processing in the context of biomedical image data or\nsimply edge detection, you may find the following material useful:\n\nDICOM processing and segmentation in Python with Scikit-Image and pydicom (Radiology Data Quest)\n\nImage manipulation and processing using Numpy and Scipy (Scipy Lecture Notes)\n\nIntensity values (presentation, DataCamp)\n\nObject detection with Raspberry Pi and Python (Maker Portal)\n\nX-ray data preparation and segmentation with deep learning (a Kaggle-hosted Jupyter notebook)\n\nImage filtering (lecture slides, CS6670: Computer Vision, Cornell University)\n\nEdge detection in Python\n\nEdge detection with Scikit-Image (Data Carpentry)\n\nImage gradients and gradient filtering (lecture slides, 16-385 Computer Vision, Carnegie Mellon University)","type":"content","url":"/tutorial-x-ray-image-processing#next-steps","position":25},{"hierarchy":{"lvl1":"Contributing"},"type":"lvl1","url":"/contributing","position":0},{"hierarchy":{"lvl1":"Contributing"},"content":"We very much welcome contributions! If you have an idea or proposal for a new\ntutorial, please \n\nopen an issue\nwith an outline.\n\nDon’t worry if English is not your first language, or if you can only come up\nwith a rough draft. Open source is a community effort. Do your best – we’ll help\nfix issues.\n\nImages and real-life data make text more engaging and powerful, but be sure what\nyou use is appropriately licensed and available. Here again, even a rough idea\nfor artwork can be polished by others.\n\nThe NumPy tutorials are a curated collection of\n\n\nMyST-NB notebooks. These notebooks are used\nto produce static websites and can be opened as notebooks in Jupyter using\n\n\nJupytext.\n\nNote: You should use \n\nCommonMark markdown\ncells. Jupyter only renders CommonMark.","type":"content","url":"/contributing","position":1},{"hierarchy":{"lvl1":"Contributing","lvl2":"Why Jupyter Notebooks?"},"type":"lvl2","url":"/contributing#why-jupyter-notebooks","position":2},{"hierarchy":{"lvl1":"Contributing","lvl2":"Why Jupyter Notebooks?"},"content":"The choice of Jupyter Notebook in this repo instead of the usual format\n(\n\nreStructuredText)\nused in the main NumPy documentation has two reasons:\n\nJupyter notebooks are a common format for communicating scientific\ninformation.\n\nJupyter notebooks can be launched in \n\nBinder, so that users can interact\nwith tutorials\n\nrST may present a barrier for some people who might otherwise be very\ninterested in contributing tutorial material.","type":"content","url":"/contributing#why-jupyter-notebooks","position":3},{"hierarchy":{"lvl1":"Contributing","lvl3":"Note","lvl2":"Why Jupyter Notebooks?"},"type":"lvl3","url":"/contributing#note","position":4},{"hierarchy":{"lvl1":"Contributing","lvl3":"Note","lvl2":"Why Jupyter Notebooks?"},"content":"You may notice our content is in markdown format (.md files). We review and\nhost notebooks in the \n\nMyST-NB format. We\naccept both Jupyter notebooks (.ipynb) and MyST-NB notebooks (.md).\nIf you want to author MyST notebooks in jupyterlab, check out the\n\n\njupyterlab_myst extension!","type":"content","url":"/contributing#note","position":5},{"hierarchy":{"lvl1":"Contributing","lvl2":"Adding your own tutorials"},"type":"lvl2","url":"/contributing#adding-your-own-tutorials","position":6},{"hierarchy":{"lvl1":"Contributing","lvl2":"Adding your own tutorials"},"content":"If you have your own tutorial in the form of a Jupyter notebook (an .ipynb\nfile) and you’d like to try add it out to the repository, follow the steps below.","type":"content","url":"/contributing#adding-your-own-tutorials","position":7},{"hierarchy":{"lvl1":"Contributing","lvl3":"Create an issue","lvl2":"Adding your own tutorials"},"type":"lvl3","url":"/contributing#create-an-issue","position":8},{"hierarchy":{"lvl1":"Contributing","lvl3":"Create an issue","lvl2":"Adding your own tutorials"},"content":"Go to \n\nhttps://​github​.com​/numpy​/numpy​-tutorials​/issues and create a new issue\nwith your proposal.\nGive as much detail as you can about what kind of content you would like to\nwrite (tutorial, how-to) and what you plan to cover.\nWe will try to respond as quickly as possible with comments, if applicable.","type":"content","url":"/contributing#create-an-issue","position":9},{"hierarchy":{"lvl1":"Contributing","lvl3":"Check out our suggested template","lvl2":"Adding your own tutorials"},"type":"lvl3","url":"/contributing#check-out-our-suggested-template","position":10},{"hierarchy":{"lvl1":"Contributing","lvl3":"Check out our suggested template","lvl2":"Adding your own tutorials"},"content":"You can use this template to make your content consistent with our existing\ntutorials.","type":"content","url":"/contributing#check-out-our-suggested-template","position":11},{"hierarchy":{"lvl1":"Contributing","lvl3":"Upload your content","lvl2":"Adding your own tutorials"},"type":"lvl3","url":"/contributing#upload-your-content","position":12},{"hierarchy":{"lvl1":"Contributing","lvl3":"Upload your content","lvl2":"Adding your own tutorials"},"content":"Remember to clear all outputs on your notebook before uploading it.\n\nFork this repository (if you haven't before).\n\nIn your own fork, create a new branch for your content.\n\nAdd your notebook to the content/ directory.\n\nUpdate the environment.yml file with the dependencies for your tutorial\n(only if you add new dependencies).\n\nUpdate this README.md to include your new entry.\n\nCreate a \n\npull request. Make sure the \"Allow edits and access to secrets by maintainers\" option is selected so we can properly review your submission.\n\n🎉 Wait for review!\n\nFor more information about GitHub and its workflow, you can see\n\n\nthis document.","type":"content","url":"/contributing#upload-your-content","position":13},{"hierarchy":{"lvl1":"NumPy tutorials"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"NumPy tutorials"},"content":"\n\nThis set of tutorials and educational materials is being developed in the\n\n\nnumpy-tutorials repository, and is\nnot a part of the NumPy source tree. The goal of this repository is to provide\nhigh-quality resources by the NumPy project, both for self-learning and for\nteaching classes with. If you’re interested in adding your own content, check\nthe \n\nContributing section.\n\nTo open a live version of the content, click the launch Binder button above.\nTo open each of the .md files, right click and select “Open with -> Notebook”.\nYou can also launch individual tutorials on Binder by clicking on the rocket\nicon that appears in the upper-right corner of each tutorial. To download a\nlocal copy of the .ipynb files, you can either\n\n\nclone this repository\nor use the download icon in the upper-right corner of each tutorial.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"NumPy tutorials","lvl3":"Non-executable articles"},"type":"lvl3","url":"/#non-executable-articles","position":2},{"hierarchy":{"lvl1":"NumPy tutorials","lvl3":"Non-executable articles"},"content":"Help improve the tutorials!\n\nWant to make a valuable contribution to the tutorials? Consider contributing to\nthese existing articles to help make them fully executable and reproducible!","type":"content","url":"/#non-executable-articles","position":3},{"hierarchy":{"lvl1":"NumPy tutorials","lvl2":"Useful links and resources"},"type":"lvl2","url":"/#useful-links-and-resources","position":4},{"hierarchy":{"lvl1":"NumPy tutorials","lvl2":"Useful links and resources"},"content":"The following links may be useful:\n\nNumPy Code of Conduct\n\nMain NumPy documentation\n\nNumPy documentation team meeting notes\n\nNEP 44 - Restructuring the NumPy documentation\n\nBlog post - Documentation as a way to build Community\n\nNote that regular documentation issues for NumPy can be found in the \n\nmain NumPy\nrepository (see the Documentation\nlabels there).","type":"content","url":"/#useful-links-and-resources","position":5}]}